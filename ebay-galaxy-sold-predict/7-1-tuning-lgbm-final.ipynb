{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "respective-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tracked-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn. neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from scipy.stats import expon, reciprocal\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from missingpy import MissForest\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-mistake",
   "metadata": {},
   "source": [
    "# Light GBM > 원핫 인코딩 안한 카테고리 변수?\n",
    "https://www.kaggle.com/mlisovyi/beware-of-categorical-features-in-lgbm\n",
    "- 카테고리 변수 지정이 의미 있는가?\n",
    "- LGBM은 연속성 실수 변수와 카테고리 변수를 어떻게 처리하는가?\n",
    "    - 토론 재밌음!\n",
    "    \n",
    "# max_bin 파라미터 튜닝 추가!\n",
    "- 값이 작으면 빨라지고,크면 느려지지만 정확해진다 \n",
    "\n",
    "# SHAP - tensorflow backed feature evaluator\n",
    "https://github.com/slundberg/shap\n",
    "- 그림이 예쁨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "timely-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(props):\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if props[col].dtype != object:  # Exclude strings\n",
    "            \n",
    "            # Print current column type\n",
    "            #print(\"******************************\")\n",
    "            #print(\"Column: \",col)\n",
    "            #print(\"dtype before: \",props[col].dtype)\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = (props[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "            \n",
    "            # Print new column type\n",
    "            #print(\"dtype after: \",props[col].dtype)\n",
    "            #print(\"******************************\")\n",
    "    \n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return props, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cloudy-acrylic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1485 entries, 0 to 1484\n",
      "Data columns (total 12 columns):\n",
      "index                        1485 non-null int64\n",
      "BuyItNow                     1485 non-null int64\n",
      "startprice                   1485 non-null float64\n",
      "color_sentiment2             1485 non-null int64\n",
      "carrier_none                 1485 non-null int64\n",
      "productSeries_imputed        1485 non-null int64\n",
      "product_isNote_imputed       1485 non-null int64\n",
      "hasDescription               1485 non-null int64\n",
      "charCountDescription         1485 non-null int64\n",
      "upperCaseDescription_rate    1485 non-null float64\n",
      "startprice_point9            1485 non-null int64\n",
      "sold                         1485 non-null int64\n",
      "dtypes: float64(2), int64(10)\n",
      "memory usage: 150.8 KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/galaxy_final_naive.csv', index_col=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-comfort",
   "metadata": {},
   "source": [
    "### Advanced Topics for LGBM\n",
    "https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html\n",
    "\n",
    "#### Categorical Feature Support\n",
    "- LightGBM offers good accuracy with integer-encoded categorical features. LightGBM applies Fisher (1958) to find the optimal split over categories as described here. This often performs better than one-hot encoding.\n",
    "- Use categorical_feature to specify the categorical features. Refer to the parameter categorical_feature in Parameters.\n",
    "- Categorical features must be encoded as non-negative integers (int) less than Int32.MaxValue (2147483647). It is best to use a contiguous range of integers started from zero.\n",
    "- Use min_data_per_group, cat_smooth to deal with over-fitting (when #data is small or #category is large).\n",
    "- For a categorical feature with high cardinality (#category is large), it often works best to treat the feature as numeric, either by simply ignoring the categorical interpretation of the integers or by embedding the categories in a low-dimensional numeric space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "comic-disposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1484.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BuyItNow</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startprice</th>\n",
       "      <td>0.01</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_sentiment2</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carrier_none</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>productSeries_imputed</th>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_isNote_imputed</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasDescription</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charCountDescription</th>\n",
       "      <td>0.00</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upperCaseDescription_rate</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startprice_point9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sold</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0       1\n",
       "index                      0.00  1484.0\n",
       "BuyItNow                   0.00     1.0\n",
       "startprice                 0.01   999.0\n",
       "color_sentiment2          -1.00     1.0\n",
       "carrier_none              -1.00     1.0\n",
       "productSeries_imputed      0.00     3.0\n",
       "product_isNote_imputed     0.00     1.0\n",
       "hasDescription             0.00     1.0\n",
       "charCountDescription       0.00   111.0\n",
       "upperCaseDescription_rate  0.00     1.0\n",
       "startprice_point9          0.00     1.0\n",
       "sold                       0.00     1.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df.min(), df.max()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "looking-zambia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 2]), array([2, 0, 1]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['color_sentiment2'] += 1\n",
    "df['carrier_none'] += 1\n",
    "df['color_sentiment2'].unique(), df['carrier_none'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "animated-convert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 0.14728546142578125  MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  0.038237571716308594  MB\n",
      "This is  25.96153846153846 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "df, NAli = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "graphic-ebony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1485 entries, 0 to 1484\n",
      "Data columns (total 12 columns):\n",
      "index                        1485 non-null uint16\n",
      "BuyItNow                     1485 non-null uint8\n",
      "startprice                   1485 non-null float32\n",
      "color_sentiment2             1485 non-null uint8\n",
      "carrier_none                 1485 non-null uint8\n",
      "productSeries_imputed        1485 non-null uint8\n",
      "product_isNote_imputed       1485 non-null uint8\n",
      "hasDescription               1485 non-null uint8\n",
      "charCountDescription         1485 non-null uint8\n",
      "upperCaseDescription_rate    1485 non-null float32\n",
      "startprice_point9            1485 non-null uint8\n",
      "sold                         1485 non-null uint8\n",
      "dtypes: float32(2), uint16(1), uint8(9)\n",
      "memory usage: 39.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "catholic-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('sold', axis=1)\n",
    "y = df.sold\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11,\n",
    "                                                       stratify=y, shuffle=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=11,\n",
    "                                                       stratify=y_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-screening",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "shaped-clark",
   "metadata": {},
   "source": [
    "![lgbm](https://cdn-images-1.medium.com/max/1000/1*A0b_ahXOrrijazzJengwYw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-radius",
   "metadata": {},
   "source": [
    "### Note\n",
    "- Similar to CatBoost, LightGBM can also handle categorical features by taking the input of feature names.\n",
    "- **It does not convert to one-hot coding, and is much faster than one-hot coding.**\n",
    "- LGBM uses a special algorithm to find the split value of categorical features [Link](http://www.csiss.org/SPACE/workshops/2004/SAC/files/fisher.pdf).\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*fR5nLi61SkS031Spb3qgLg.png)\n",
    "\n",
    "- Note: You should convert your categorical features to int type before you construct Dataset for LGBM. It does not accept string values even if you passes it through categorical_feature parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-proposal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "stone-george",
   "metadata": {},
   "source": [
    "# Target Encoding\n",
    "### XAM / Additive Smoothing / Feature Extraction.\n",
    "```bash\n",
    "pip install fabric3\n",
    "pip install git+https://github.com/MaxHalford/xam --upgrade\n",
    "```\n",
    "- https://maxhalford.github.io/blog/target-encoding/\n",
    "    - https://github.com/MaxHalford/xam/blob/master/docs/feature-extraction.md#smooth-target-encoding\n",
    "- https://www.wikiwand.com/en/Additive_smoothing\n",
    "\n",
    "### Light GBM / CatBoost\n",
    "- Optimal Binning (최적구간으로 연속형 실수 등 숫자 칼럼을 쪼갬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "induced-spank",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x_0       x_1\n",
       "0  0.6875  0.541667\n",
       "1  0.6875  0.541667\n",
       "2  0.6875  0.541667\n",
       "3  0.6875  0.541667\n",
       "4  0.6875  0.541667\n",
       "5  0.3125  0.541667\n",
       "6  0.3125  0.541667\n",
       "7  0.3125  0.541667\n",
       "8  0.3125  0.541667\n",
       "9  0.3125  0.375000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xam\n",
    "XX = pd.DataFrame({'x_0': ['a'] * 5 + ['b'] * 5, 'x_1': ['a'] * 9 + ['b'] * 1})\n",
    "yy = pd.Series([1, 1, 1, 1, 0, 1, 0, 0, 0, 0])\n",
    "\n",
    "encoder = xam.feature_extraction.BayesianTargetEncoder(\n",
    "     columns=['x_0', 'x_1'],\n",
    "     prior_weight=3,\n",
    "     suffix=''\n",
    ")\n",
    "encoder.fit_transform(XX, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-trouble",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "suited-wrist",
   "metadata": {},
   "source": [
    "# Gentle Intro to Light GBM for applied ML\n",
    "https://sefiks.com/2018/10/13/a-gentle-introduction-to-lightgbm-for-applied-machine-learning/\n",
    "\n",
    "Light GBM은 어떤 이유로 엄청난 인기를 얻게 되었을까요?\n",
    "- 데이터 사이즈는 날이 갈수록 커지고 있고 전통적인 데이터 분석 알고리즘으로 빠른 결과를 얻기란 더욱 어려워졌습니다. Light GBM은 말 그대로 “Light” 가벼운 것인데요, 왜냐면 속도가 빠르기 때문입니다. Light GBM은 큰 사이즈의 데이터를 다룰 수 있고 실행시킬 때 적은 메모리를 차지합니다. Light GBM이 인기있는 또 다른 이유는 바로 결과의 정확도에 초점을 맞추기 때문입니다. LGBM은 또한 GPU 학습을 지원하기 때문에 데이터 사이언티스트가 데이터 분석 어플리케이션을 개발할 때 LGBM을 폭넓게 사용하고 있습니다.\n",
    "\n",
    "Light GBM은 어디서나 사용할 수 있을까요?\n",
    "- 아닙니다. LGBM을 작은 데이터 세트에 사용하는 것은 추천되지 않습니다. Light GBM은 overfitting (과적합)에 민감하고 작은 데이터에 대해서 과적합하기 쉽습니다. row (행) 수에 대한 제한은 없지만 제 경험상 10,000 이상의 row (행) 을 가진 데이터에 사용하는 것을 권유해드립니다.\n",
    "\n",
    "### Params & default values\n",
    "https://testlightgbm.readthedocs.io/en/latest/Parameters.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-world",
   "metadata": {},
   "source": [
    "### First Grid\n",
    "Earliest / Major Param\n",
    "- scaler\n",
    "- learning rate\n",
    "- boosting type\n",
    "- num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "designed-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical\n",
    "cat_features_idx = [(i) for (i, colname) in enumerate((df.columns)) if (len(df[colname].unique()) <= 3)\n",
    "                   & (colname != 'sold')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "honest-wrist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   45.0s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:  6.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': LGBMClassifier(bagging_fraction=0.8, boosting_type='dart',\n",
      "               categorical_feature=[1, 3, 4, 6, 7, 10], class_weight=None,\n",
      "               colsample_bytree=1.0, drop_rate=0.1, feature_fraction=0.8,\n",
      "               importance_type='split', lambda_l1=0, lambda_l2=0,\n",
      "               learning_rate=0.01, max_bin=255, max_depth=5,\n",
      "               metric='binary_logloss', min_child_samples=20,\n",
      "               min_child_weight=0.001, min_data_in_leaf=20, min_split_gain=0.0,\n",
      "               n_estimators=1000, n_jobs=-1, num_iterations=5000, num_leaves=8,\n",
      "               objective='binary', random_state=None, reg_alpha=0.0,\n",
      "               reg_lambda=0.0, scale_pos_weight=1.0, silent=True, skip_drop=0.5,\n",
      "               subsample=1.0, ...), 'classifier__bagging_fraction': 0.8, 'classifier__boosting_type': 'dart', 'classifier__categorical_feature': [1, 3, 4, 6, 7, 10], 'classifier__drop_rate': 0.1, 'classifier__feature_fraction': 0.8, 'classifier__lambda_l1': 0, 'classifier__lambda_l2': 0, 'classifier__learning_rate': 0.01, 'classifier__max_bin': 255, 'classifier__max_depth': 5, 'classifier__metric': 'binary_logloss', 'classifier__min_data_in_leaf': 20, 'classifier__n_estimators': 1000, 'classifier__num_iterations': 5000, 'classifier__num_leaves': 8, 'classifier__objective': 'binary', 'classifier__scale_pos_weight': 1.0, 'classifier__skip_drop': 0.5, 'scale': RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "             with_scaling=True)}\n",
      "0.8200000000000001\n"
     ]
    }
   ],
   "source": [
    "# without PolynomialFeatures/ feature selection\n",
    "\n",
    "pipe = Pipeline([\n",
    "                ('scale', MinMaxScaler()),\n",
    "#                 ('poly', PolynomialFeatures()),\n",
    "#                 ('feature_selection', RFE(LGBMClassifier())),\n",
    "                ('classifier', LGBMClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid1 = [              \n",
    "              {'classifier': [LGBMClassifier()],\n",
    "               'classifier__categorical_feature':[cat_features_idx],\n",
    "               'classifier__objective':['binary'],\n",
    "               'classifier__metric':['binary_logloss'],\n",
    "              'classifier__boosting_type':['gbdt', 'dart'],\n",
    "              'classifier__drop_rate':[0.1],\n",
    "               'classifier__skip_drop':[0.5],\n",
    "               'classifier__learning_rate':[0.01, 0.03, 0.1],\n",
    "               'classifier__num_iterations':[500, 1000, 2000, 3000, 5000],\n",
    "              'classifier__bagging_fraction': [0.8],\n",
    "               'classifier__feature_fraction':[0.8],\n",
    "               'classifier__early_stopping_round':[0],\n",
    "               'classifier__max_depth': [5],\n",
    "               'classifier__num_leaves':[2**3],\n",
    "               'classifier__min_data_in_leaf':[20],\n",
    "               'classifier__max_bin':[255],\n",
    "               'classifier__n_estimators':[1000],\n",
    "               'classifier__lambda_l1':[0],\n",
    "               'classifier__lambda_l2':[0],\n",
    "               'classifier__scale_pos_weight':[1.0],\n",
    "               'scale':[MinMaxScaler(), StandardScaler(), RobustScaler()],\n",
    "#                'poly':[PolynomialFeatures()],\n",
    "#                'poly__degree':[3],\n",
    "#               'feature_selection' : [RFE(LGBMClassifier(objective='binary',\n",
    "#                                                         metric='binary_logloss'))],\n",
    "#                 'feature_selection__n_features_to_select' : [140, 70, 35]\n",
    "              }\n",
    "             ]\n",
    "grid1 = GridSearchCV(pipe, param_grid1, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid1.fit(X_train, y_train)\n",
    "print(grid1.best_params_)\n",
    "print(grid1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-establishment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "boxed-handy",
   "metadata": {},
   "source": [
    "### Second Grid\n",
    "major hyper parameters\n",
    "- drop_rate\n",
    "- skip_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "united-capture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=-1)]: Done 405 out of 405 | elapsed: 33.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': LGBMClassifier(bagging_fraction=0.8, boosting_type='dart',\n",
      "               categorical_feature=[1, 3, 4, 6, 7, 10], class_weight=None,\n",
      "               colsample_bytree=1.0, drop_rate=0.3500000000000001,\n",
      "               early_stopping_round=0, feature_fraction=0.8,\n",
      "               importance_type='split', lambda_l1=0, lambda_l2=0,\n",
      "               learning_rate=0.01, max_bin=255, max_depth=5,\n",
      "               metric='binary_logloss', min_child_samples=20,\n",
      "               min_child_weight=0.001, min_data_in_leaf=20, min_split_gain=0.0,\n",
      "               n_estimators=1000, n_jobs=-1, num_iterations=5000, num_leaves=8,\n",
      "               objective='binary', random_state=None, reg_alpha=0.0,\n",
      "               reg_lambda=0.0, scale_pos_weight=1.0, silent=True,\n",
      "               skip_drop=0.45000000000000007, ...), 'classifier__bagging_fraction': 0.8, 'classifier__boosting_type': 'dart', 'classifier__categorical_feature': [1, 3, 4, 6, 7, 10], 'classifier__drop_rate': 0.3500000000000001, 'classifier__early_stopping_round': 0, 'classifier__feature_fraction': 0.8, 'classifier__lambda_l1': 0, 'classifier__lambda_l2': 0, 'classifier__learning_rate': 0.01, 'classifier__max_bin': 255, 'classifier__max_depth': 5, 'classifier__metric': 'binary_logloss', 'classifier__min_data_in_leaf': 20, 'classifier__n_estimators': 1000, 'classifier__num_iterations': 5000, 'classifier__num_leaves': 8, 'classifier__objective': 'binary', 'classifier__scale_pos_weight': 1.0, 'classifier__skip_drop': 0.45000000000000007, 'scale': RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "             with_scaling=True)}\n",
      "0.8231578947368421\n"
     ]
    }
   ],
   "source": [
    "# drop_Rate, skip_drop\n",
    "\n",
    "pipe = Pipeline([\n",
    "                ('scale', RobustScaler()),\n",
    "#                 ('poly', PolynomialFeatures()),\n",
    "#                 ('feature_selection', RFE(LGBMClassifier())),\n",
    "                ('classifier', LGBMClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid2 = [              \n",
    "              {'classifier': [LGBMClassifier()],\n",
    "               'classifier__categorical_feature':[cat_features_idx],\n",
    "               'classifier__objective':['binary'],\n",
    "               'classifier__metric':['binary_logloss'],\n",
    "              'classifier__boosting_type':[grid1.best_params_['classifier__boosting_type']],\n",
    "              'classifier__drop_rate':np.arange(0.1, 0.55, 0.05),\n",
    "               'classifier__skip_drop':np.arange(0.1, 0.55, 0.05),\n",
    "               'classifier__learning_rate':[grid1.best_params_['classifier__learning_rate']],\n",
    "               'classifier__num_iterations':[grid1.best_params_['classifier__num_iterations']],\n",
    "              'classifier__bagging_fraction': [0.8],\n",
    "               'classifier__feature_fraction':[0.8],\n",
    "               'classifier__early_stopping_round':[0],\n",
    "               'classifier__max_depth': [5],\n",
    "               'classifier__num_leaves':[2**3],\n",
    "               'classifier__min_data_in_leaf':[20],\n",
    "               'classifier__max_bin':[255],\n",
    "               'classifier__n_estimators':[1000],\n",
    "               'classifier__lambda_l1':[0],\n",
    "               'classifier__lambda_l2':[0],\n",
    "               'classifier__scale_pos_weight':[1.0],\n",
    "               'scale':[RobustScaler()],\n",
    "#                'poly':[PolynomialFeatures()],\n",
    "#                'poly__degree':[3],\n",
    "#               'feature_selection' : [RFE(LGBMClassifier(objective='binary',\n",
    "#                                                         metric='binary_logloss'))],\n",
    "#                 'feature_selection__n_features_to_select' : [140, 70, 35]\n",
    "              }\n",
    "             ]\n",
    "grid2 = GridSearchCV(pipe, param_grid2, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid2.fit(X_train, y_train)\n",
    "print(grid2.best_params_)\n",
    "print(grid2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-enterprise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "interesting-automation",
   "metadata": {},
   "source": [
    "### Third\n",
    "- max_depth\n",
    "- num_leaves\n",
    "- min_data_in_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "derived-scientist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed: 21.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': LGBMClassifier(bagging_fraction=0.8, boosting_type='dart',\n",
      "               categorical_feature=[1, 3, 4, 6, 7, 10], class_weight=None,\n",
      "               colsample_bytree=1.0, drop_rate=0.3500000000000001,\n",
      "               early_stopping_round=0, feature_fraction=0.8,\n",
      "               importance_type='split', lambda_l1=0, lambda_l2=0,\n",
      "               learning_rate=0.01, max_bin=255, max_depth=6,\n",
      "               metric='binary_logloss', min_child_samples=20,\n",
      "               min_child_weight=0.001, min_data_in_leaf=20, min_split_gain=0.0,\n",
      "               n_estimators=1000, n_jobs=-1, num_iterations=5000, num_leaves=7,\n",
      "               objective='binary', random_state=None, reg_alpha=0.0,\n",
      "               reg_lambda=0.0, scale_pos_weight=1.0, silent=True,\n",
      "               skip_drop=0.45000000000000007, ...), 'classifier__bagging_fraction': 0.8, 'classifier__boosting_type': 'dart', 'classifier__categorical_feature': [1, 3, 4, 6, 7, 10], 'classifier__drop_rate': 0.3500000000000001, 'classifier__early_stopping_round': 0, 'classifier__feature_fraction': 0.8, 'classifier__lambda_l1': 0, 'classifier__lambda_l2': 0, 'classifier__learning_rate': 0.01, 'classifier__max_bin': 255, 'classifier__max_depth': 6, 'classifier__metric': 'binary_logloss', 'classifier__min_data_in_leaf': 20, 'classifier__n_estimators': 1000, 'classifier__num_iterations': 5000, 'classifier__num_leaves': 7, 'classifier__objective': 'binary', 'classifier__scale_pos_weight': 1.0, 'classifier__skip_drop': 0.45000000000000007, 'scale': RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "             with_scaling=True)}\n",
      "0.82\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "                ('scale', RobustScaler()),\n",
    "#                 ('poly', PolynomialFeatures()),\n",
    "#                 ('feature_selection', RFE(LGBMClassifier())),\n",
    "                ('classifier', LGBMClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid3 = [              \n",
    "              {'classifier': [LGBMClassifier()],\n",
    "               'classifier__categorical_feature':[cat_features_idx],\n",
    "               'classifier__objective':['binary'],\n",
    "               'classifier__metric':['binary_logloss'],\n",
    "              'classifier__boosting_type':[grid1.best_params_['classifier__boosting_type']],\n",
    "              'classifier__drop_rate':[grid2.best_params_['classifier__drop_rate']],\n",
    "               'classifier__skip_drop':[grid2.best_params_['classifier__skip_drop']],\n",
    "               'classifier__learning_rate':[grid1.best_params_['classifier__learning_rate']],\n",
    "               'classifier__num_iterations':[grid1.best_params_['classifier__num_iterations']],\n",
    "              'classifier__bagging_fraction': [0.8],\n",
    "               'classifier__feature_fraction':[0.8],\n",
    "               'classifier__max_depth': [3, 6, 9, 12, 15, 20],\n",
    "               'classifier__num_leaves':[2**3-1, 2**5-1, 2**7-1, 2**9-1],\n",
    "               'classifier__min_data_in_leaf':[20, 100, 250, 500, 1000],\n",
    "               'classifier__max_bin':[255],\n",
    "               'classifier__n_estimators':[1000],\n",
    "               'classifier__early_stopping_round':[0],\n",
    "               'classifier__lambda_l1':[0],\n",
    "               'classifier__lambda_l2':[0],\n",
    "               'classifier__scale_pos_weight':[1.0],\n",
    "               'scale':[RobustScaler()],\n",
    "#                'poly':[PolynomialFeatures()],\n",
    "#                'poly__degree':[3],\n",
    "#               'feature_selection' : [RFE(LGBMClassifier(objective='binary',\n",
    "#                                                         metric='binary_logloss'))],\n",
    "#                 'feature_selection__n_features_to_select' : [140, 70, 35]\n",
    "              }\n",
    "             ]\n",
    "\n",
    "grid3 = GridSearchCV(pipe, param_grid3, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid3.fit(X_train, y_train)\n",
    "print(grid3.best_params_)\n",
    "print(grid3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-smith",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "welsh-spider",
   "metadata": {},
   "source": [
    "### Fourth Grid\n",
    "major hyper parameters\n",
    "- bagging_fraction\n",
    "    - bagging_freq\n",
    "- feature_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "amazing-prague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 125 candidates, totalling 625 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 24.0min\n",
      "[Parallel(n_jobs=-1)]: Done 625 out of 625 | elapsed: 33.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': LGBMClassifier(bagging_fraction=0.5, bagging_freq=1000, boosting_type='dart',\n",
      "               categorical_feature=[1, 3, 4, 6, 7, 10], class_weight=None,\n",
      "               colsample_bytree=1.0, drop_rate=0.3500000000000001,\n",
      "               early_stopping_round=0, feature_fraction=0.9,\n",
      "               importance_type='split', lambda_l1=0, lambda_l2=0,\n",
      "               learning_rate=0.01, max_bin=255, max_depth=6,\n",
      "               metric='binary_logloss', min_child_samples=20,\n",
      "               min_child_weight=0.001, min_data_in_leaf=20, min_split_gain=0.0,\n",
      "               n_estimators=1000, n_jobs=-1, num_iterations=5000, num_leaves=7,\n",
      "               objective='binary', random_state=None, reg_alpha=0.0,\n",
      "               reg_lambda=0.0, scale_pos_weight=1.0, silent=True, ...), 'classifier__bagging_fraction': 0.5, 'classifier__bagging_freq': 1000, 'classifier__boosting_type': 'dart', 'classifier__categorical_feature': [1, 3, 4, 6, 7, 10], 'classifier__drop_rate': 0.3500000000000001, 'classifier__early_stopping_round': 0, 'classifier__feature_fraction': 0.9, 'classifier__lambda_l1': 0, 'classifier__lambda_l2': 0, 'classifier__learning_rate': 0.01, 'classifier__max_bin': 255, 'classifier__max_depth': 6, 'classifier__metric': 'binary_logloss', 'classifier__min_data_in_leaf': 20, 'classifier__n_estimators': 1000, 'classifier__num_iterations': 5000, 'classifier__num_leaves': 7, 'classifier__objective': 'binary', 'classifier__scale_pos_weight': 1.0, 'classifier__skip_drop': 0.45000000000000007, 'scale': RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "             with_scaling=True)}\n",
      "0.8284210526315791\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "                ('scale', RobustScaler()),\n",
    "#                 ('poly', PolynomialFeatures()),\n",
    "#                 ('feature_selection', RFE(LGBMClassifier())),\n",
    "                ('classifier', LGBMClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid4 = [              \n",
    "              {'classifier': [LGBMClassifier()],\n",
    "               'classifier__categorical_feature':[cat_features_idx],\n",
    "               'classifier__objective':['binary'],\n",
    "               'classifier__metric':['binary_logloss'],\n",
    "              'classifier__boosting_type':[grid1.best_params_['classifier__boosting_type']],\n",
    "              'classifier__drop_rate':[grid2.best_params_['classifier__drop_rate']],\n",
    "               'classifier__skip_drop':[grid2.best_params_['classifier__skip_drop']],\n",
    "               'classifier__learning_rate':[grid1.best_params_['classifier__learning_rate']],\n",
    "               'classifier__num_iterations':[grid1.best_params_['classifier__num_iterations']],\n",
    "              'classifier__bagging_fraction': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "               'classifier__bagging_freq':[100, 250, 500, 750, 1000],\n",
    "               'classifier__feature_fraction':[0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "               'classifier__max_depth': [grid3.best_params_['classifier__max_depth']],\n",
    "               'classifier__num_leaves':[grid3.best_params_['classifier__num_leaves']],\n",
    "               'classifier__min_data_in_leaf':[grid3.best_params_['classifier__min_data_in_leaf']],\n",
    "               'classifier__max_bin':[255],\n",
    "               'classifier__n_estimators':[1000],\n",
    "               'classifier__early_stopping_round':[0],\n",
    "               'classifier__lambda_l1':[0],\n",
    "               'classifier__lambda_l2':[0],\n",
    "               'classifier__scale_pos_weight':[1.0],\n",
    "               'scale':[RobustScaler()],\n",
    "#                'poly':[PolynomialFeatures()],\n",
    "#                'poly__degree':[3],\n",
    "#               'feature_selection' : [RFE(LGBMClassifier(objective='binary',\n",
    "#                                                         metric='binary_logloss'))],\n",
    "#                 'feature_selection__n_features_to_select' : [140, 70, 35]\n",
    "              }\n",
    "             ]\n",
    "grid4 = GridSearchCV(pipe, param_grid4, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid4.fit(X_train, y_train)\n",
    "print(grid4.best_params_)\n",
    "print(grid4.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-symphony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pleased-terminal",
   "metadata": {},
   "source": [
    "### Fifth\n",
    "- max_bin\n",
    "- n_estimators\n",
    "- early_stopping_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "posted-singapore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 24.7min\n",
      "[Parallel(n_jobs=-1)]: Done 750 out of 750 | elapsed: 41.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': LGBMClassifier(bagging_fraction=0.5, bagging_freq=1000, boosting_type='dart',\n",
      "               categorical_feature=[1, 3, 4, 6, 7, 10], class_weight=None,\n",
      "               colsample_bytree=1.0, drop_rate=0.3500000000000001,\n",
      "               early_stopping_round=10, feature_fraction=0.9,\n",
      "               importance_type='split', lambda_l1=0, lambda_l2=0,\n",
      "               learning_rate=0.01, max_bin=150, max_depth=6,\n",
      "               metric='binary_logloss', min_child_samples=20,\n",
      "               min_child_weight=0.001, min_data_in_leaf=20, min_split_gain=0.0,\n",
      "               n_estimators=250, n_jobs=-1, num_iterations=5000, num_leaves=7,\n",
      "               objective='binary', random_state=None, reg_alpha=0.0,\n",
      "               reg_lambda=0.0, scale_pos_weight=1.0, silent=True, ...), 'classifier__bagging_fraction': 0.5, 'classifier__bagging_freq': 1000, 'classifier__boosting_type': 'dart', 'classifier__categorical_feature': [1, 3, 4, 6, 7, 10], 'classifier__drop_rate': 0.3500000000000001, 'classifier__early_stopping_round': 10, 'classifier__feature_fraction': 0.9, 'classifier__lambda_l1': 0, 'classifier__lambda_l2': 0, 'classifier__learning_rate': 0.01, 'classifier__max_bin': 150, 'classifier__max_depth': 6, 'classifier__metric': 'binary_logloss', 'classifier__min_data_in_leaf': 20, 'classifier__n_estimators': 250, 'classifier__num_iterations': 5000, 'classifier__num_leaves': 7, 'classifier__objective': 'binary', 'classifier__scale_pos_weight': 1.0, 'classifier__skip_drop': 0.45000000000000007, 'scale': RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "             with_scaling=True)}\n",
      "0.8305263157894738\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "                ('scale', RobustScaler()),\n",
    "#                 ('poly', PolynomialFeatures()),\n",
    "#                 ('feature_selection', RFE(LGBMClassifier())),\n",
    "                ('classifier', LGBMClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid5 = [              \n",
    "              {'classifier': [LGBMClassifier()],\n",
    "               'classifier__categorical_feature':[cat_features_idx],\n",
    "               'classifier__objective':['binary'],\n",
    "               'classifier__metric':['binary_logloss'],\n",
    "              'classifier__boosting_type':[grid1.best_params_['classifier__boosting_type']],\n",
    "              'classifier__drop_rate':[grid2.best_params_['classifier__drop_rate']],\n",
    "               'classifier__skip_drop':[grid2.best_params_['classifier__skip_drop']],\n",
    "               'classifier__learning_rate':[grid1.best_params_['classifier__learning_rate']],\n",
    "               'classifier__num_iterations':[grid1.best_params_['classifier__num_iterations']],\n",
    "              'classifier__bagging_fraction': [grid4.best_params_['classifier__bagging_fraction']],\n",
    "               'classifier__bagging_freq': [grid4.best_params_['classifier__bagging_freq']],\n",
    "               'classifier__feature_fraction':[grid4.best_params_['classifier__feature_fraction']],\n",
    "               'classifier__max_depth': [grid3.best_params_['classifier__max_depth']],\n",
    "               'classifier__num_leaves': [grid3.best_params_['classifier__num_leaves']],\n",
    "               'classifier__min_data_in_leaf': [grid3.best_params_['classifier__min_data_in_leaf']],\n",
    "               'classifier__max_bin':[100, 150, 200, 255, 300],\n",
    "               'classifier__n_estimators':[250, 500, 1000, 2000, 3000],\n",
    "               'classifier__early_stopping_round':[10, 25, 50, 75, 100, 200],\n",
    "               'classifier__lambda_l1':[0],\n",
    "               'classifier__lambda_l2':[0],\n",
    "               'classifier__scale_pos_weight':[1.0],\n",
    "               'scale':[RobustScaler()],\n",
    "#                'poly':[PolynomialFeatures()],\n",
    "#                'poly__degree':[3],\n",
    "#               'feature_selection' : [RFE(LGBMClassifier(objective='binary',\n",
    "#                                                         metric='binary_logloss'))],\n",
    "#                 'feature_selection__n_features_to_select' : [140, 70, 35]\n",
    "              }\n",
    "             ]\n",
    "\n",
    "grid5 = GridSearchCV(pipe, param_grid5, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid5.fit(X_train, y_train)\n",
    "print(grid5.best_params_)\n",
    "print(grid5.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-salmon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "oriental-customer",
   "metadata": {},
   "source": [
    "### Sixth\n",
    "- lambda_l1\n",
    "- lambda_l2\n",
    "- scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "unlikely-scope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 24.6min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 43.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1080 out of 1080 | elapsed: 59.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': LGBMClassifier(bagging_fraction=0.5, bagging_freq=1000, boosting_type='dart',\n",
      "               categorical_feature=[1, 3, 4, 6, 7, 10], class_weight=None,\n",
      "               colsample_bytree=1.0, drop_rate=0.3500000000000001,\n",
      "               early_stopping_round=10, feature_fraction=0.9,\n",
      "               importance_type='split', lambda_l1=0.01, lambda_l2=0.001,\n",
      "               learning_rate=0.01, max_bin=150, max_depth=6,\n",
      "               metric='binary_logloss', min_child_samples=20,\n",
      "               min_child_weight=0.001, min_data_in_leaf=20, min_split_gain=0.0,\n",
      "               n_estimators=250, n_jobs=-1, num_iterations=5000, num_leaves=7,\n",
      "               objective='binary', random_state=None, reg_alpha=0.0,\n",
      "               reg_lambda=0.0, scale_pos_weight=1.0, silent=True, ...), 'classifier__bagging_fraction': 0.5, 'classifier__bagging_freq': 1000, 'classifier__boosting_type': 'dart', 'classifier__categorical_feature': [1, 3, 4, 6, 7, 10], 'classifier__drop_rate': 0.3500000000000001, 'classifier__early_stopping_round': 10, 'classifier__feature_fraction': 0.9, 'classifier__lambda_l1': 0.01, 'classifier__lambda_l2': 0.001, 'classifier__learning_rate': 0.01, 'classifier__max_bin': 150, 'classifier__max_depth': 6, 'classifier__metric': 'binary_logloss', 'classifier__min_data_in_leaf': 20, 'classifier__n_estimators': 250, 'classifier__num_iterations': 5000, 'classifier__num_leaves': 7, 'classifier__objective': 'binary', 'classifier__scale_pos_weight': 1.0, 'classifier__skip_drop': 0.45000000000000007, 'scale': RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "             with_scaling=True)}\n",
      "0.8357894736842105\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "                ('scale', RobustScaler()),\n",
    "#                 ('poly', PolynomialFeatures()),\n",
    "#                 ('feature_selection', RFE(LGBMClassifier())),\n",
    "                ('classifier', LGBMClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid6 = [              \n",
    "              {'classifier': [LGBMClassifier()],\n",
    "               'classifier__categorical_feature':[cat_features_idx],\n",
    "               'classifier__objective':['binary'],\n",
    "               'classifier__metric':['binary_logloss'],\n",
    "              'classifier__boosting_type':[grid1.best_params_['classifier__boosting_type']],\n",
    "              'classifier__drop_rate':[grid2.best_params_['classifier__drop_rate']],\n",
    "               'classifier__skip_drop':[grid2.best_params_['classifier__skip_drop']],\n",
    "               'classifier__learning_rate':[grid1.best_params_['classifier__learning_rate']],\n",
    "               'classifier__num_iterations':[grid1.best_params_['classifier__num_iterations']],\n",
    "              'classifier__bagging_fraction': [grid4.best_params_['classifier__bagging_fraction']],\n",
    "               'classifier__bagging_freq': [grid4.best_params_['classifier__bagging_freq']],\n",
    "               'classifier__feature_fraction':[grid4.best_params_['classifier__feature_fraction']],\n",
    "               'classifier__max_depth': [grid3.best_params_['classifier__max_depth']],\n",
    "               'classifier__num_leaves': [grid3.best_params_['classifier__num_leaves']],\n",
    "               'classifier__min_data_in_leaf': [grid3.best_params_['classifier__min_data_in_leaf']],\n",
    "               'classifier__max_bin':[grid5.best_params_['classifier__max_bin']],\n",
    "               'classifier__n_estimators':[grid5.best_params_['classifier__n_estimators']],\n",
    "               'classifier__early_stopping_round':[grid5.best_params_['classifier__early_stopping_round']],\n",
    "               'classifier__lambda_l1':[0, 1e-4, 1e-3, 1e-2, 0.1, 1],\n",
    "               'classifier__lambda_l2':[0, 1e-4, 1e-3, 1e-2, 0.1, 1],\n",
    "               'classifier__scale_pos_weight':[1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
    "               'scale':[RobustScaler()],\n",
    "#                'poly':[PolynomialFeatures()],\n",
    "#                'poly__degree':[3],\n",
    "#               'feature_selection' : [RFE(LGBMClassifier(objective='binary',\n",
    "#                                                         metric='binary_logloss'))],\n",
    "#                 'feature_selection__n_features_to_select' : [140, 70, 35]\n",
    "              }\n",
    "             ]\n",
    "\n",
    "grid6 = GridSearchCV(pipe, param_grid6, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid6.fit(X_train, y_train)\n",
    "print(grid6.best_params_)\n",
    "print(grid6.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-gamma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "mathematical-spell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'classifier': [LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "                  importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "                  min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "                  n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "                  random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "                  subsample=1.0, subsample_for_bin=200000, subsample_freq=0)],\n",
       "  'classifier__categorical_feature': [[1, 3, 4, 6, 7, 10]],\n",
       "  'classifier__objective': ['binary'],\n",
       "  'classifier__metric': ['binary_logloss'],\n",
       "  'classifier__boosting_type': ['dart'],\n",
       "  'classifier__drop_rate': [0.3500000000000001],\n",
       "  'classifier__skip_drop': [0.45000000000000007],\n",
       "  'classifier__learning_rate': [0.01],\n",
       "  'classifier__num_iterations': [5000],\n",
       "  'classifier__bagging_fraction': [0.5],\n",
       "  'classifier__bagging_freq': [1000],\n",
       "  'classifier__feature_fraction': [0.9],\n",
       "  'classifier__max_depth': [6],\n",
       "  'classifier__num_leaves': [7],\n",
       "  'classifier__min_data_in_leaf': [20],\n",
       "  'classifier__max_bin': [150],\n",
       "  'classifier__n_estimators': [250],\n",
       "  'classifier__early_stopping_round': [10],\n",
       "  'classifier__lambda_l1': [0.01],\n",
       "  'classifier__lambda_l2': [0.001],\n",
       "  'classifier__scale_pos_weight': [1.0]}]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_fin = [              \n",
    "              {'classifier': [LGBMClassifier()],\n",
    "               'classifier__categorical_feature':[cat_features_idx],\n",
    "               'classifier__objective':['binary'],\n",
    "               'classifier__metric':['binary_logloss'],\n",
    "              'classifier__boosting_type':[grid1.best_params_['classifier__boosting_type']],\n",
    "              'classifier__drop_rate':[grid2.best_params_['classifier__drop_rate']],\n",
    "               'classifier__skip_drop':[grid2.best_params_['classifier__skip_drop']],\n",
    "               'classifier__learning_rate':[grid1.best_params_['classifier__learning_rate']],\n",
    "               'classifier__num_iterations':[grid1.best_params_['classifier__num_iterations']],\n",
    "              'classifier__bagging_fraction': [grid4.best_params_['classifier__bagging_fraction']],\n",
    "               'classifier__bagging_freq': [grid4.best_params_['classifier__bagging_freq']],\n",
    "               'classifier__feature_fraction':[grid4.best_params_['classifier__feature_fraction']],\n",
    "               'classifier__max_depth': [grid3.best_params_['classifier__max_depth']],\n",
    "               'classifier__num_leaves': [grid3.best_params_['classifier__num_leaves']],\n",
    "               'classifier__min_data_in_leaf': [grid3.best_params_['classifier__min_data_in_leaf']],\n",
    "               'classifier__max_bin':[grid5.best_params_['classifier__max_bin']],\n",
    "               'classifier__n_estimators':[grid5.best_params_['classifier__n_estimators']],\n",
    "               'classifier__early_stopping_round':[grid5.best_params_['classifier__early_stopping_round']],\n",
    "               'classifier__lambda_l1':[grid6.best_params_['classifier__lambda_l1']],\n",
    "               'classifier__lambda_l2':[grid6.best_params_['classifier__lambda_l2']],\n",
    "               'classifier__scale_pos_weight':[grid6.best_params_['classifier__scale_pos_weight']]\n",
    "              }]\n",
    "\n",
    "param_grid_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "remarkable-identification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8047138047138047"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_best = LGBMClassifier(categorical_feature=[1, 3, 4, 6, 7, 10],\n",
    "                          objective='binary', metric='binary_logloss',\n",
    "                          boosting_type='dart', drop_rate=0.35, skip_drop=0.45,\n",
    "                           learning_rate=0.01, num_iterations=5000,\n",
    "                           subsample=0.5, subsample_freq=1000,\n",
    "                           colsample_bytree=0.9, max_depth=6, num_leaves=7,\n",
    "                           min_child_samples=20, max_bin=150, n_estimators=250,\n",
    "                           early_stopping_round=10, reg_alpha=0.01, reg_lambda=0.001,\n",
    "                           scale_pos_weight=1.0, verbose=-1\n",
    "                      )\n",
    "\n",
    "sc = RobustScaler()\n",
    "Xtr = sc.fit_transform(X_train)\n",
    "Xte = sc.transform(X_test)\n",
    "\n",
    "lgbm_best.fit(Xtr, y_train)\n",
    "accuracy_score(y_test, lgbm_best.predict(Xte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-participation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-nomination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "brief-johnston",
   "metadata": {},
   "source": [
    "# Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "national-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lof2(model, df, \n",
    "                  scaler=None, poly=None, dim_reduction=None, rfe=None,\n",
    "                  preset=False):    \n",
    "    best_params, best_acc = 0, 0  \n",
    "    test_neighbors = np.linspace(1, 31, num=30).astype(int)\n",
    "    test_contams = np.linspace(0.01, 0.26, num=25)\n",
    "    \n",
    "    if preset:\n",
    "        X0_train, X0_valid, X0_test, y0_train, y0_valid, y0_test = df\n",
    "        \n",
    "    else:\n",
    "        X0 = df.drop('sold', axis=1)\n",
    "        y0 = df.sold\n",
    "        X0_train, X0_test, y0_train, y0_test = train_test_split(X0, y0,\n",
    "                                                                test_size=0.2,\n",
    "                                                                shuffle=True,\n",
    "                                                                stratify=y0,\n",
    "                                                                random_state=11)\n",
    "        X0_train, X0_valid, y0_train, y0_valid = train_test_split(X0_train, y0_train,\n",
    "                                                                test_size=0.2,\n",
    "                                                                shuffle=True,\n",
    "                                                                stratify=y0_train,\n",
    "                                                                random_state=11)\n",
    "\n",
    "        if scaler:\n",
    "            X0_train = scaler.fit_transform(X0_train)\n",
    "            X0_valid = scaler.transform(X0_valid)\n",
    "            X0_test = scaler.transform(X0_test)\n",
    "\n",
    "        if poly:\n",
    "            X0_train = poly.fit_transform(X0_train)\n",
    "            X0_valid = poly.transform(X0_valid)\n",
    "            X0_test = poly.transform(X0_test)\n",
    "\n",
    "        if dim_reduction:\n",
    "            X0_train = dim_reduction.fit_transform(X0_train)\n",
    "            X0_valid = dim_reduction.transform(X0_valid)\n",
    "            X0_test = dim_reduction.transform(X0_test)\n",
    "\n",
    "        if rfe:\n",
    "            X0_train = rfe.fit_transform(X0_train, y0_train)\n",
    "            X0_valid = rfe.transform(X0_valid)\n",
    "            X0_test = rfe.transform(X0_test)\n",
    "\n",
    "        print('preprocessing complete')\n",
    "    \n",
    "    for i, tn in enumerate(test_neighbors):\n",
    "        print(i, end='/')\n",
    "        for j, tc in enumerate(test_contams):\n",
    "            \n",
    "            # 원본 보존을 위해 복사본 사용\n",
    "            X_train_copy, X_valid_copy, X_test_copy = X0_train.copy(), X0_valid.copy(), X0_test.copy()\n",
    "            y_train_copy, y_valid_copy, y_test_copy = y0_train.copy(), y0_valid.copy(), y0_test.copy()\n",
    "\n",
    "            # LOF 모델 생성 및 트레인셋 학습\n",
    "            clf = LocalOutlierFactor(n_neighbors=tn, contamination=tc,\n",
    "                                    novelty=True, n_jobs=-1)\n",
    "            clf.fit(X_train_copy)\n",
    "\n",
    "            # 트레인셋 아웃라이어 제거\n",
    "            y_pred = clf.predict(X_train_copy)\n",
    "            lof_outlier_idx_train = pd.Series(y_pred)[pd.Series(y_pred)==-1].index\n",
    "            X_train_copy = pd.DataFrame(X_train_copy).reset_index(drop=True).drop(lof_outlier_idx_train)\n",
    "            y_train_copy = y_train_copy.reset_index(drop=True).drop(lof_outlier_idx_train)\n",
    "\n",
    "            # 밸리데이션 셋 아웃라이어 제거\n",
    "#             yval_pred = clf.predict(X_valid_copy)\n",
    "#             lof_outlier_idx_valid = pd.Series(yval_pred)[pd.Series(yval_pred)==-1].index\n",
    "#             X_valid_copy = pd.DataFrame(X_valid_copy).reset_index(drop=True).drop(lof_outlier_idx_valid)\n",
    "#             y_valid_copy = y_valid_copy.reset_index(drop=True).drop(lof_outlier_idx_valid)\n",
    "\n",
    "            # 테스트 셋 아웃라이어 제거\n",
    "#             ytest_pred = clf.predict(X_test_copy)\n",
    "#             lof_outlier_idx_test = pd.Series(ytest_pred)[pd.Series(ytest_pred)==-1].index\n",
    "#             X_test_copy = pd.DataFrame(X_test_copy).reset_index(drop=True).drop(lof_outlier_idx_test)\n",
    "#             y_test_copy = y_test_copy.reset_index(drop=True).drop(lof_outlier_idx_test)\n",
    "\n",
    "            # 예측모델 정의 및 트레인 셋으로 학습\n",
    "            mod = model\n",
    "            mod.fit(X_train_copy, y_train_copy, verbose=-1)\n",
    "\n",
    "            # scaling만 적용한 밸리데이션 셋으로 정확도 측정\n",
    "            mod_acc = accuracy_score(y_valid_copy, mod.predict(X_valid_copy))\n",
    "            \n",
    "            if best_acc < mod_acc:\n",
    "                best_acc = mod_acc\n",
    "                best_params = (tn, tc)\n",
    "                print((tn, tc, best_acc), end='/')\n",
    "#                 X2 = X2\n",
    "#                 y2 = y2\n",
    "                \n",
    "    \n",
    "    return {'best_params':best_params,\n",
    "           'best_accuracy':best_acc,\n",
    "           'preprocessed_data':[X0_train, X0_valid, X0_test, y0_train, y0_valid, y0_test],\n",
    "           'LOF_data':[X_train_copy, X_valid_copy, X_test_copy,\n",
    "                      y_train_copy, y_valid_copy, y_test_copy]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "elect-forty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing complete\n",
      "0/(1, 0.01, 0.7647058823529411)/1/(2, 0.01, 0.7773109243697479)/(2, 0.020416666666666666, 0.8067226890756303)/(2, 0.09333333333333332, 0.8151260504201681)/(2, 0.14541666666666667, 0.819327731092437)/(2, 0.26, 0.8235294117647058)/2/3/4/5/6/7/8/9/10/11/12/13/14/15/16/17/18/19/20/21/22/23/24/25/26/27/28/29/"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2, 0.26), 0.8235294117647058)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting parameter name for sklearn wrapper version LGBM\n",
    "lgbm_best = LGBMClassifier(categorical_feature=[1, 3, 4, 6, 7, 10],\n",
    "                          objective='binary', metric='binary_logloss',\n",
    "                          boosting_type='dart', drop_rate=0.35, skip_drop=0.45,\n",
    "                           learning_rate=0.01, num_iterations=5000,\n",
    "                           subsample=0.5, subsample_freq=1000,\n",
    "                           colsample_bytree=0.9, max_depth=6, num_leaves=7,\n",
    "                           min_child_samples=20, max_bin=150, n_estimators=250,\n",
    "                           early_stopping_round=10, reg_alpha=0.01, reg_lambda=0.001,\n",
    "                           scale_pos_weight=1.0, verbose=-1\n",
    "                      )\n",
    "lgbm_scaler = RobustScaler()\n",
    "lgbm_lof_tune = tune_lof2(lgbm_best, df,\n",
    "                              scaler=lgbm_scaler)\n",
    "lgbm_lof_tune['best_params'], lgbm_lof_tune['best_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-green",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "combined-rapid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5546218487394958"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_scaler = RobustScaler()\n",
    "X_train_rs = lgbm_scaler.fit_transform(X_train)\n",
    "X_test_rs = lgbm_scaler.transform(X_test)\n",
    "\n",
    "clf = LocalOutlierFactor(n_neighbors=2, contamination=0.26,\n",
    "                                    novelty=True, n_jobs=-1)\n",
    "clf.fit(X_train_rs)\n",
    "y_pred = clf.predict(X_train_rs)\n",
    "\n",
    "lof_outlier_idx_train = pd.Series(y_pred)[pd.Series(y_pred)==-1].index\n",
    "X_train_lof = pd.DataFrame(X_train_rs).reset_index(drop=True).drop(lof_outlier_idx_train)\n",
    "y_train_lof = y_train.reset_index(drop=True).drop(lof_outlier_idx_train)\n",
    "\n",
    "lgbm_best = LGBMClassifier(categorical_feature=[1, 3, 4, 6, 7, 10],\n",
    "                          objective='binary', metric='binary_logloss',\n",
    "                          boosting_type='dart', drop_rate=0.35, skip_drop=0.45,\n",
    "                           learning_rate=0.01, num_iterations=5000,\n",
    "                           subsample=0.5, subsample_freq=1000,\n",
    "                           colsample_bytree=0.9, max_depth=6, num_leaves=7,\n",
    "                           min_child_samples=20, max_bin=150, n_estimators=250,\n",
    "                           early_stopping_round=10, reg_alpha=0.01, reg_lambda=0.001,\n",
    "                           scale_pos_weight=1.0, verbose=-1\n",
    "                      )\n",
    "\n",
    "lgbm_best.fit(X_train_lof, y_train_lof)\n",
    "# accuracy_score(y_test, lgbm_best.predict(X_test))\n",
    "accuracy_score(y_valid, lgbm_best.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "favorite-wagner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5488215488215489"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-struggle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-evanescence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "separate-wisconsin",
   "metadata": {},
   "source": [
    "# 최적 LOF 적용한 트레인셋 -> 2차 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-world",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-independence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-blend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-violation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "specified-craps",
   "metadata": {},
   "source": [
    "# Last Try\n",
    "- LGBM은 하이퍼 파라미터 튜닝으로, outlier 포함된 데이터에 FIT 된 상태 (마른걸레 짜기)\n",
    "- outlier를 제거하였으므로 모델과 데이터 간의 FIT은 감소했다고 생각할 수 있음\n",
    "- LGBM 오버핏 제어 파라미터를 조정하여 조금 더 robust한 상태로 LOF data를 학습시켜 보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier_by_lof(lof_fit, Xdf, ydf):\n",
    "    ypred = lof_fit.predict(Xdf)\n",
    "    lof_outlier_idx = pd.Series(ypred)[pd.Series(ypred)==-1].index\n",
    "    Xdf_ = pd.DataFrame(Xdf).reset_index(drop=True).drop(lof_outlier_idx)\n",
    "    ydf_ = ydf.reset_index(drop=True).drop(lof_outlier_idx)\n",
    "    return Xdf_, ydf_\n",
    "\n",
    "rscaler = RobustScaler()\n",
    "X_train_ = rscaler.fit_transform(X_train)\n",
    "X_valid_ = rscaler.transform(X_valid)\n",
    "X_test_ = rscaler.transform(X_test)\n",
    "\n",
    "clf = LocalOutlierFactor(n_neighbors=tn, contamination=tc,\n",
    "                        novelty=True, n_jobs=-1)\n",
    "clf.fit(X_train_copy)\n",
    "\n",
    "Xtrain_lof, ytrain_lof = remove_outlier_by_lof(clf, X_train_, y_train)\n",
    "Xvalid_lof, yvalid_lof = remove_outlier_by_lof(clf, X_valid_, y_valid)\n",
    "Xtest_lof, ytest_lof = remove_outlier_by_lof(clf, X_test_, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "#                 ('scale', RobustScaler()),\n",
    "#                 ('poly', PolynomialFeatures()),\n",
    "#                 ('feature_selection', RFE(LGBMClassifier())),\n",
    "                ('classifier', LGBMClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid6 = [              \n",
    "              {'classifier': [LGBMClassifier()],\n",
    "               'classifier__categorical_feature':[cat_features_idx],\n",
    "               'classifier__objective':['binary'],\n",
    "               'classifier__metric':['binary_logloss'],\n",
    "              'classifier__boosting_type':[grid1.best_params_['classifier__boosting_type']],\n",
    "              'classifier__drop_rate':[grid2.best_params_['classifier__drop_rate']],\n",
    "               'classifier__skip_drop':[grid2.best_params_['classifier__skip_drop']],\n",
    "               'classifier__learning_rate':[grid1.best_params_['classifier__learning_rate']],\n",
    "               'classifier__num_iterations':[grid1.best_params_['classifier__num_iterations']],\n",
    "              'classifier__bagging_fraction': [grid4.best_params_['classifier__bagging_fraction']],\n",
    "               'classifier__bagging_freq': [grid4.best_params_['classifier__bagging_freq']],\n",
    "               'classifier__feature_fraction':[grid4.best_params_['classifier__feature_fraction']],\n",
    "               'classifier__max_depth': [grid3.best_params_['classifier__max_depth']],\n",
    "               'classifier__num_leaves': [grid3.best_params_['classifier__num_leaves']],\n",
    "               'classifier__min_data_in_leaf': [grid3.best_params_['classifier__min_data_in_leaf']],\n",
    "               'classifier__max_bin':[grid5.best_params_['classifier__max_bin']],\n",
    "               'classifier__n_estimators':[grid5.best_params_['classifier__n_estimators']],\n",
    "               'classifier__early_stopping_round':[grid5.best_params_['classifier__early_stopping_round']],\n",
    "               'classifier__lambda_l1':[0, 1e-4, 1e-3, 1e-2, 0.1, 1],\n",
    "               'classifier__lambda_l2':[0, 1e-4, 1e-3, 1e-2, 0.1, 1],\n",
    "               'classifier__scale_pos_weight':[1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
    "#                'scale':[RobustScaler()],\n",
    "#                'poly':[PolynomialFeatures()],\n",
    "#                'poly__degree':[3],\n",
    "#               'feature_selection' : [RFE(LGBMClassifier(objective='binary',\n",
    "#                                                         metric='binary_logloss'))],\n",
    "#                 'feature_selection__n_features_to_select' : [140, 70, 35]\n",
    "              }\n",
    "             ]\n",
    "\n",
    "grid6 = GridSearchCV(pipe, param_grid6, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid6.fit(X_train, y_train)\n",
    "print(grid6.best_params_)\n",
    "print(grid6.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-compensation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-field",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-brisbane",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-pathology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dominant-identification",
   "metadata": {},
   "source": [
    "# Stage 2:\n",
    "\n",
    "Poly + RFE 적용\n",
    "\n",
    "```\n",
    "model.fit(categorical_feature=[0, 1, ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "                ('scale', MinMaxScaler()),\n",
    "                 ('poly', PolynomialFeatures()),\n",
    "                ('feature_selection', RFE(XGBClassifier())),\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid1 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': [0.15],\n",
    "              'classifier__skip_drop': [0.33],\n",
    "             'classifier__learning_rate':[0.1],\n",
    "             'classifier__n_estimators':[1000],\n",
    "               'classifier__max_depth':[5],\n",
    "               'classifier__min_child_weight':[1],\n",
    "               'classifier__gamma':[0],\n",
    "               'classifier__subsample':[0.8],\n",
    "               'classifier__colsample_bytree':[0.8],\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1],\n",
    "               'scale':[MinMaxScaler()],\n",
    "               'poly':[PolynomialFeatures()],\n",
    "               'poly__degree': [3],\n",
    "               'feature_selection' : [RFE(XGBClassifier(objective='binary:logistic',\n",
    "                                                       eval_metric='error'))],\n",
    "               'feature_selection__n_features_to_select' : [140, 70, 35, 18]\n",
    "#                'reduce_dims' : [PCA(), LDA(), TSNE()],\n",
    "#                'reduce_dims__n_components' : [5, 7, 9, 11]\n",
    "              }\n",
    "             ]\n",
    "grid1 = GridSearchCV(pipe1, param_grid1, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid1.fit(X_valid, y_valid)\n",
    "print(grid1.best_params_)\n",
    "print(grid1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-better",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-depression",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-shift",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
