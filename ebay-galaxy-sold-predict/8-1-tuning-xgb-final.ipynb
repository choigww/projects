{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "written-organizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_dev/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn. neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from scipy.stats import expon, reciprocal\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from missingpy import MissForest\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "marked-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(props):\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if props[col].dtype != object:  # Exclude strings\n",
    "            \n",
    "            # Print current column type\n",
    "            #print(\"******************************\")\n",
    "            #print(\"Column: \",col)\n",
    "            #print(\"dtype before: \",props[col].dtype)\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = (props[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "            \n",
    "            # Print new column type\n",
    "            #print(\"dtype after: \",props[col].dtype)\n",
    "            #print(\"******************************\")\n",
    "    \n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return props, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "productive-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 0.16994476318359375  MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  0.041069984436035156  MB\n",
      "This is  24.166666666666668 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "df, NAli = reduce_mem_usage(pd.read_csv('./data/galaxy_final.csv', index_col=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "conventional-conversion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1485 entries, 0 to 1484\n",
      "Data columns (total 14 columns):\n",
      "index                        1485 non-null uint16\n",
      "BuyItNow                     1485 non-null uint8\n",
      "startprice                   1485 non-null float32\n",
      "productSeries_imputed        1485 non-null uint8\n",
      "product_isNote_imputed       1485 non-null uint8\n",
      "hasDescription               1485 non-null uint8\n",
      "charCountDescriptionBins     1485 non-null uint8\n",
      "upperCaseDescription_rate    1485 non-null float32\n",
      "startprice_point9            1485 non-null uint8\n",
      "sold                         1485 non-null uint8\n",
      "color_sentiment_0            1485 non-null uint8\n",
      "color_sentiment_1            1485 non-null uint8\n",
      "carrier_none_0               1485 non-null uint8\n",
      "carrier_none_1               1485 non-null uint8\n",
      "dtypes: float32(2), uint16(1), uint8(11)\n",
      "memory usage: 42.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "infrared-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('sold', axis=1)\n",
    "y = df.sold\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11,\n",
    "                                                       stratify=y, shuffle=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=11,\n",
    "                                                       stratify=y_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crucial-beginning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 33.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(base_score=None, booster='dart', colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8, eval_metric='error',\n",
      "              gamma=0, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.1,\n",
      "              max_delta_step=None, max_depth=5, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "              nthread=-1, num_parallel_tree=None, objective='binary:logistic',\n",
      "              random_state=None, rate_drop=0.15, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=1, seed=2021, skip_drop=0.33,\n",
      "              subsample=0.8, tree_method=None, ...), 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__min_child_weight': 1, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.15, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.33, 'classifier__subsample': 0.8, 'feature_selection': RFE(estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                            colsample_bylevel=None, colsample_bynode=None,\n",
      "                            colsample_bytree=None, eval_metric='error',\n",
      "                            gamma=None, gpu_id=None, importance_type='gain',\n",
      "                            interaction_constraints=None, learning_rate=None,\n",
      "                            max_delta_step=None, max_depth=None,\n",
      "                            min_child_weight=None, missing=nan,\n",
      "                            monotone_constraints=None, n_estimators=100,\n",
      "                            n_jobs=None, num_parallel_tree=None,\n",
      "                            objective='binary:logistic', random_state=None,\n",
      "                            reg_alpha=None, reg_lambda=None,\n",
      "                            scale_pos_weight=None, subsample=None,\n",
      "                            tree_method=None, use_label_encoder=True,\n",
      "                            validate_parameters=None, verbosity=None),\n",
      "    n_features_to_select=70, step=1, verbose=0), 'feature_selection__n_features_to_select': 70, 'poly': PolynomialFeatures(degree=3, include_bias=True, interaction_only=False,\n",
      "                   order='C'), 'poly__degree': 3, 'scale': MinMaxScaler(copy=True, feature_range=(0, 1))}\n",
      "0.7477836879432623\n"
     ]
    }
   ],
   "source": [
    "pipe1 = Pipeline([\n",
    "                ('scale', MinMaxScaler()),\n",
    "                 ('poly', PolynomialFeatures()),\n",
    "                ('feature_selection', RFE(XGBClassifier())),\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid1 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': [0.15],\n",
    "              'classifier__skip_drop': [0.33],\n",
    "             'classifier__learning_rate':[0.1],\n",
    "             'classifier__n_estimators':[1000],\n",
    "               'classifier__max_depth':[5],\n",
    "               'classifier__min_child_weight':[1],\n",
    "               'classifier__gamma':[0],\n",
    "               'classifier__subsample':[0.8],\n",
    "               'classifier__colsample_bytree':[0.8],\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1],\n",
    "               'scale':[MinMaxScaler()],\n",
    "               'poly':[PolynomialFeatures()],\n",
    "               'poly__degree': [3],\n",
    "               'feature_selection' : [RFE(XGBClassifier(objective='binary:logistic',\n",
    "                                                       eval_metric='error'))],\n",
    "               'feature_selection__n_features_to_select' : [140, 70, 35, 18]\n",
    "#                'reduce_dims' : [PCA(), LDA(), TSNE()],\n",
    "#                'reduce_dims__n_components' : [5, 7, 9, 11]\n",
    "              }\n",
    "             ]\n",
    "grid1 = GridSearchCV(pipe1, param_grid1, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid1.fit(X_valid, y_valid)\n",
    "print(grid1.best_params_)\n",
    "print(grid1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "horizontal-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "rfe = RFE(XGBClassifier(objective='binary:logistic', eval_metric='error'),\n",
    "          n_features_to_select=70)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_valid = poly.transform(X_valid)\n",
    "X_test = poly.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dynamic-student",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 33s, sys: 9.08 s, total: 29min 43s\n",
      "Wall time: 8min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = rfe.fit_transform(X_train, y_train)\n",
    "X_valid = rfe.transform(X_valid)\n",
    "X_test = rfe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "corporate-house",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of columns:70\n"
     ]
    }
   ],
   "source": [
    "print(f'num of columns:{len(X_train[0, :])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "wound-convergence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 105 candidates, totalling 525 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 29.7min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 64.8min\n",
      "[Parallel(n_jobs=-1)]: Done 525 out of 525 | elapsed: 78.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(base_score=None, booster='dart', colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8, eval_metric='error',\n",
      "              gamma=0, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.02,\n",
      "              max_delta_step=None, max_depth=9, min_child_weight=4, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "              nthread=-1, num_parallel_tree=None, objective='binary:logistic',\n",
      "              random_state=None, rate_drop=0.15, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=1, seed=2021, skip_drop=0.33,\n",
      "              subsample=0.8, tree_method=None, ...), 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 9, 'classifier__min_child_weight': 4, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.15, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.33, 'classifier__subsample': 0.8}\n",
      "0.8221052631578948\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid2 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': [0.15],\n",
    "              'classifier__skip_drop': [0.33],\n",
    "             'classifier__learning_rate':[0.01, 0.02, 0.03],\n",
    "             'classifier__n_estimators':[1000],\n",
    "               'classifier__max_depth':range(3,10),\n",
    "               'classifier__min_child_weight':range(1,6),\n",
    "               'classifier__gamma':[0],\n",
    "               'classifier__subsample':[0.8],\n",
    "               'classifier__colsample_bytree':[0.8],\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1],\n",
    "              }\n",
    "             ]\n",
    "grid2 = GridSearchCV(pipe, param_grid2, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid2.fit(X_train, y_train)\n",
    "print(grid2.best_params_)\n",
    "print(grid2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-magnet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "intensive-division",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  9.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(base_score=None, booster='dart', colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8, eval_metric='error',\n",
      "              gamma=0.0, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.02,\n",
      "              max_delta_step=None, max_depth=9, min_child_weight=4, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "              nthread=-1, num_parallel_tree=None, objective='binary:logistic',\n",
      "              random_state=None, rate_drop=0.15, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=1, seed=2021, skip_drop=0.33,\n",
      "              subsample=0.8, tree_method=None, ...), 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0.0, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 9, 'classifier__min_child_weight': 4, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.15, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.33, 'classifier__subsample': 0.8}\n",
      "0.8221052631578948\n"
     ]
    }
   ],
   "source": [
    "# Gamma 튜닝\n",
    "\n",
    "pipe = Pipeline([\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid3 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': [0.15],\n",
    "              'classifier__skip_drop': [0.33],\n",
    "             'classifier__learning_rate':[grid2.best_params_['classifier__learning_rate']],\n",
    "             'classifier__n_estimators':[1000],\n",
    "               'classifier__max_depth': [grid2.best_params_['classifier__max_depth']], # 3\n",
    "               'classifier__min_child_weight': [grid2.best_params_['classifier__min_child_weight']],# 3\n",
    "               'classifier__gamma':[i/10.0 for i in range(0,10)],\n",
    "               'classifier__subsample':[0.8],\n",
    "               'classifier__colsample_bytree':[0.8],\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1],\n",
    "#                'scale':[MinMaxScaler()],\n",
    "#                'poly':[PolynomialFeatures()],\n",
    "#                'poly__degree':[3],\n",
    "#                'feature_selection' : [RFE(RandomForestClassifier())],\n",
    "#                'feature_selection__n_features_to_select' : [140]\n",
    "              }\n",
    "             ]\n",
    "grid3 = GridSearchCV(pipe, param_grid3, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid3.fit(X_train, y_train)\n",
    "print(grid3.best_params_)\n",
    "print(grid3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-representation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "precious-panama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed: 13.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(base_score=None, booster='dart', colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8, eval_metric='error',\n",
      "              gamma=0.0, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.02,\n",
      "              max_delta_step=None, max_depth=9, min_child_weight=4, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "              nthread=-1, num_parallel_tree=None, objective='binary:logistic',\n",
      "              random_state=None, rate_drop=0.15, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=1, seed=2021, skip_drop=0.33,\n",
      "              subsample=0.8, tree_method=None, ...), 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0.0, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 9, 'classifier__min_child_weight': 4, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.15, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.33, 'classifier__subsample': 0.8}\n",
      "0.8221052631578948\n"
     ]
    }
   ],
   "source": [
    "# subsample and colsample_bytree를 튜닝한다.\n",
    "\n",
    "pipe= Pipeline([\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid4 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': [0.15],\n",
    "              'classifier__skip_drop': [0.33],\n",
    "             'classifier__learning_rate':[grid2.best_params_['classifier__learning_rate']],\n",
    "             'classifier__n_estimators':[1000],\n",
    "               'classifier__max_depth': [grid2.best_params_['classifier__max_depth']], # 3\n",
    "               'classifier__min_child_weight': [grid2.best_params_['classifier__min_child_weight']],# 3\n",
    "               'classifier__gamma':[grid3.best_params_['classifier__gamma']], # 0.5\n",
    "               'classifier__subsample':[i/10.0 for i in range(6,10)],\n",
    "               'classifier__colsample_bytree':[i/10.0 for i in range(6,10)],\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1]\n",
    "              }\n",
    "             ]\n",
    "grid4 = GridSearchCV(pipe, param_grid4, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid4.fit(X_train, y_train)\n",
    "print(grid4.best_params_)\n",
    "print(grid4.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-camera",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "moving-closer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(base_score=None, booster='dart', colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8, eval_metric='error',\n",
      "              gamma=0.0, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.02,\n",
      "              max_delta_step=None, max_depth=9, min_child_weight=4, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "              nthread=-1, num_parallel_tree=None, objective='binary:logistic',\n",
      "              random_state=None, rate_drop=0.15, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=1, seed=2021, skip_drop=0.33,\n",
      "              subsample=0.8, tree_method=None, ...), 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0.0, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 9, 'classifier__min_child_weight': 4, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.15, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.33, 'classifier__subsample': 0.8}\n",
      "0.8221052631578948\n"
     ]
    }
   ],
   "source": [
    "# n_estimator 튜닝\n",
    "\n",
    "pipe = Pipeline([\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid5 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': [0.15],\n",
    "              'classifier__skip_drop': [0.33],\n",
    "             'classifier__learning_rate':[grid2.best_params_['classifier__learning_rate']],\n",
    "             'classifier__n_estimators':[100, 200, 300, 400, 500, 700, 1000],\n",
    "               'classifier__max_depth': [grid2.best_params_['classifier__max_depth']], # 3\n",
    "               'classifier__min_child_weight': [grid2.best_params_['classifier__min_child_weight']],# 3\n",
    "               'classifier__gamma':[grid3.best_params_['classifier__gamma']], # 0.5\n",
    "               'classifier__subsample':[grid4.best_params_['classifier__subsample']], # 0.8\n",
    "               'classifier__colsample_bytree':[grid4.best_params_['classifier__colsample_bytree']], # 0.8\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1],\n",
    "\n",
    "              }\n",
    "             ]\n",
    "grid5 = GridSearchCV(pipe, param_grid5, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid5.fit(X_train, y_train)\n",
    "print(grid5.best_params_)\n",
    "print(grid5.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-asbestos",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "sharp-ceiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 33.8min\n",
      "[Parallel(n_jobs=-1)]: Done 405 out of 405 | elapsed: 66.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(base_score=None, booster='dart', colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8, eval_metric='error',\n",
      "              gamma=0.0, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.02,\n",
      "              max_delta_step=None, max_depth=9, min_child_weight=4, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "              nthread=-1, num_parallel_tree=None, objective='binary:logistic',\n",
      "              random_state=None, rate_drop=0.20000000000000004, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=1, seed=2021,\n",
      "              skip_drop=0.30000000000000004, subsample=0.8, tree_method=None, ...), 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0.0, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 9, 'classifier__min_child_weight': 4, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.20000000000000004, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.30000000000000004, 'classifier__subsample': 0.8}\n",
      "0.8221052631578948\n"
     ]
    }
   ],
   "source": [
    "# rate_drop, skip_drop 튜닝\n",
    "\n",
    "pipe = Pipeline([\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid6 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': np.arange(0.1, 0.55, 0.05),\n",
    "              'classifier__skip_drop': np.arange(0.1, 0.55, 0.05),\n",
    "               'classifier__learning_rate':[grid2.best_params_['classifier__learning_rate']], # \n",
    "             'classifier__n_estimators':[grid5.best_params_['classifier__n_estimators']], # \n",
    "               'classifier__max_depth': [grid2.best_params_['classifier__max_depth']], # 3\n",
    "               'classifier__min_child_weight': [grid2.best_params_['classifier__min_child_weight']],# 3\n",
    "               'classifier__gamma':[grid3.best_params_['classifier__gamma']], # 0.5\n",
    "               'classifier__subsample':[grid4.best_params_['classifier__subsample']], # 0.8\n",
    "               'classifier__colsample_bytree':[grid4.best_params_['classifier__colsample_bytree']], # 0.8\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1],\n",
    "              }\n",
    "             ]\n",
    "grid6 = GridSearchCV(pipe, param_grid6, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid6.fit(X_train, y_train)\n",
    "print(grid6.best_params_)\n",
    "print(grid6.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-material",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "coated-estonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 29.8min\n",
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed: 37.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(alpha=0.0001, base_score=None, booster='dart',\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, eval_metric='error', gamma=0.0, gpu_id=None,\n",
      "              importance_type='gain', interaction_constraints=None, lambda=1,\n",
      "              learning_rate=0.02, max_delta_step=None, max_depth=9,\n",
      "              min_child_weight=4, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=1000, n_jobs=-1, nthread=-1, num_parallel_tree=None,\n",
      "              objective='binary:logistic', random_state=None,\n",
      "              rate_drop=0.20000000000000004, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=1, seed=2021, skip_drop=0.30000000000000004, ...), 'classifier__alpha': 0.0001, 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0.0, 'classifier__lambda': 1, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 9, 'classifier__min_child_weight': 4, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.20000000000000004, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.30000000000000004, 'classifier__subsample': 0.8}\n",
      "0.8221052631578948\n"
     ]
    }
   ],
   "source": [
    "# 정규화 상수 튜닝\n",
    "\n",
    "pipe = Pipeline([\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid7 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': [grid6.best_params_['classifier__rate_drop']],\n",
    "              'classifier__skip_drop': [grid6.best_params_['classifier__skip_drop']],\n",
    "               'classifier__learning_rate':[grid2.best_params_['classifier__learning_rate']], # \n",
    "             'classifier__n_estimators':[grid5.best_params_['classifier__n_estimators']], # \n",
    "               'classifier__max_depth': [grid2.best_params_['classifier__max_depth']], # 3\n",
    "               'classifier__min_child_weight': [grid2.best_params_['classifier__min_child_weight']],# 3\n",
    "               'classifier__gamma':[grid3.best_params_['classifier__gamma']], # 0.5\n",
    "               'classifier__subsample':[grid4.best_params_['classifier__subsample']], # 0.8\n",
    "               'classifier__colsample_bytree':[grid4.best_params_['classifier__colsample_bytree']], # 0.8\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1],\n",
    "               'classifier__alpha':[0, 1e-4, 1e-3, 1e-2, 0.1, 1],\n",
    "               'classifier__lambda':[1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100]\n",
    "              }\n",
    "             ]\n",
    "grid7 = GridSearchCV(pipe, param_grid7, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid7.fit(X_train, y_train)\n",
    "print(grid7.best_params_)\n",
    "print(grid7.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-complex",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "usual-assault",
   "metadata": {},
   "source": [
    "# Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "color-divorce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/(10, 0.8215488215488216)\n",
      "1/(20, 0.835016835016835)\n",
      "2/3/4/5/6/7/8/9/10/11/12/13/14/15/16/17/18/19/20/21/22/23/24/25/26/27/28/29/30/31/32/33/34/35/36/37/(380, 0.8417508417508418)\n",
      "38/39/40/41/42/43/44/45/46/47/48/49/"
     ]
    }
   ],
   "source": [
    "xgb_best = grid7.best_params_['classifier']\n",
    "best_esr_stoprounds_rfe = (-1, -1)\n",
    "for i, esr in enumerate(np.arange(10, 510, 10)):\n",
    "    print(i, end='/')\n",
    "    xgb_best = grid7.best_params_['classifier']\n",
    "    xgb_best.fit(X_train, y_train, early_stopping_rounds=esr, eval_metric=\"error\",\n",
    "                 eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=0)\n",
    "    acc = accuracy_score(y_test, xgb_best.predict(X_test))\n",
    "    \n",
    "    if acc > best_esr_stoprounds_rfe[1]:\n",
    "        best_esr_stoprounds_rfe = (esr, acc)\n",
    "        print(best_esr_stoprounds_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-bermuda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "level-entity",
   "metadata": {},
   "source": [
    "# Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "novel-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lof_xgb2(model, df, stoprounds=50,\n",
    "                  scaler=None, poly=None, dim_reduction=None, rfe=None,\n",
    "                  preset=False):    \n",
    "    best_params, best_acc = 0, 0  \n",
    "    test_neighbors = np.linspace(1, 51, num=50).astype(int)\n",
    "    test_contams = np.linspace(0.01, 0.3, num=30)\n",
    "    metrics = ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
    "      'manhattan']\n",
    "    \n",
    "    if preset:\n",
    "        X0_train, X0_valid, X0_test, y0_train, y0_valid, y0_test = df\n",
    "        \n",
    "    else:\n",
    "        X0 = df.drop('sold', axis=1)\n",
    "        y0 = df.sold\n",
    "        X0_train, X0_test, y0_train, y0_test = train_test_split(X0, y0,\n",
    "                                                                test_size=0.2,\n",
    "                                                                shuffle=True,\n",
    "                                                                stratify=y0,\n",
    "                                                                random_state=11)\n",
    "        X0_train, X0_valid, y0_train, y0_valid = train_test_split(X0_train, y0_train,\n",
    "                                                                test_size=0.2,\n",
    "                                                                shuffle=True,\n",
    "                                                                stratify=y0_train,\n",
    "                                                                random_state=11)\n",
    "\n",
    "        if scaler:\n",
    "            X0_train = scaler.fit_transform(X0_train)\n",
    "            X0_valid = scaler.transform(X0_valid)\n",
    "            X0_test = scaler.transform(X0_test)\n",
    "\n",
    "        if poly:\n",
    "            X0_train = poly.fit_transform(X0_train)\n",
    "            X0_valid = poly.transform(X0_valid)\n",
    "            X0_test = poly.transform(X0_test)\n",
    "\n",
    "        if dim_reduction:\n",
    "            X0_train = dim_reduction.fit_transform(X0_train)\n",
    "            X0_valid = dim_reduction.transform(X0_valid)\n",
    "            X0_test = dim_reduction.transform(X0_test)\n",
    "\n",
    "        if rfe:\n",
    "            X0_train = rfe.fit_transform(X0_train, y0_train)\n",
    "            X0_valid = rfe.transform(X0_valid)\n",
    "            X0_test = rfe.transform(X0_test)\n",
    "\n",
    "        print('preprocessing complete')\n",
    "    \n",
    "    for i, tn in enumerate(test_neighbors):\n",
    "        for j, tc in enumerate(test_contams):\n",
    "            print(i, j, end=' / ')\n",
    "            for m in metrics:\n",
    "            \n",
    "                # 원본 보존을 위해 복사본 사용\n",
    "                X_train_copy, X_valid_copy, X_test_copy = X0_train.copy(), X0_valid.copy(), X0_test.copy()\n",
    "                y_train_copy, y_valid_copy, y_test_copy = y0_train.copy(), y0_valid.copy(), y0_test.copy()\n",
    "\n",
    "                # LOF 모델 생성 및 트레인셋 학습\n",
    "                clf = LocalOutlierFactor(n_neighbors=tn, contamination=tc,\n",
    "                                        novelty=True, metric = m,n_jobs=-1)\n",
    "                clf.fit(X_train_copy)\n",
    "\n",
    "                # 트레인셋 아웃라이어 제거\n",
    "                y_pred = clf.predict(X_train_copy)\n",
    "                lof_outlier_idx_train = pd.Series(y_pred)[pd.Series(y_pred)==-1].index\n",
    "                X_train_copy = pd.DataFrame(X_train_copy).drop(lof_outlier_idx_train)\n",
    "                y_train_copy = y_train_copy.reset_index(drop=True).drop(lof_outlier_idx_train)\n",
    "\n",
    "                # 밸리데이션 셋 아웃라이어 제거\n",
    "                yval_pred = clf.predict(X_valid_copy)\n",
    "                lof_outlier_idx_valid = pd.Series(yval_pred)[pd.Series(yval_pred)==-1].index\n",
    "                X_valid_copy = pd.DataFrame(X_valid_copy).drop(lof_outlier_idx_valid)\n",
    "                y_valid_copy = y_valid_copy.reset_index(drop=True).drop(lof_outlier_idx_valid)\n",
    "\n",
    "                # 테스트 셋 아웃라이어 제거\n",
    "                ytest_pred = clf.predict(X_test_copy)\n",
    "                lof_outlier_idx_test = pd.Series(ytest_pred)[pd.Series(ytest_pred)==-1].index\n",
    "                X_test_copy = pd.DataFrame(X_test_copy).drop(lof_outlier_idx_test)\n",
    "                y_test_copy = y_test_copy.reset_index(drop=True).drop(lof_outlier_idx_test)\n",
    "\n",
    "                # 예측모델 정의 및 트레인/벨리데이션 셋으로 학습\n",
    "                mod = model\n",
    "                mod.fit(X_train_copy, y_train_copy, early_stopping_rounds=stoprounds, eval_metric=\"error\",\n",
    "                     eval_set=[(X_train_copy, y_train_copy), (X_valid_copy, y_valid_copy)], verbose=0)\n",
    "\n",
    "                # 테스트 정확도 측정 및 최고기록 업데이트\n",
    "                mod_acc = accuracy_score(y_test_copy, mod.predict(X_test_copy))\n",
    "                if best_acc < mod_acc:\n",
    "                    best_acc = mod_acc\n",
    "                    best_params = (tn, tc, m)\n",
    "    #                 X2 = X2\n",
    "    #                 y2 = y2\n",
    "                    print((tn, tc, m, best_acc))\n",
    "    \n",
    "    return {'best_params':best_params,\n",
    "           'best_accuracy':best_acc,\n",
    "           'preprocessed_data':[X0_train, X0_valid, X0_test, y0_train, y0_valid, y0_test],\n",
    "           'LOF_data':[X_train_copy, X_valid_copy, X_test_copy,\n",
    "                      y_train_copy, y_valid_copy, y_test_copy]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "color-convert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.0001, base_score=0.5, booster='dart', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, eval_metric='error',\n",
       "              gamma=0.0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', lambda=1, learning_rate=0.02,\n",
       "              max_delta_step=0, max_depth=9, min_child_weight=4, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=1000, n_jobs=-1,\n",
       "              nthread=-1, num_parallel_tree=1, objective='binary:logistic',\n",
       "              random_state=2021, rate_drop=0.20000000000000004,\n",
       "              reg_alpha=9.99999975e-05, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=2021, skip_drop=0.30000000000000004, ...)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid7.best_params_['classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "complimentary-programming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 / (1, 0.01, 'cityblock', 0.8412162162162162)\n",
      "0 1 / (1, 0.019999999999999997, 'euclidean', 0.8430034129692833)\n",
      "0 2 / 0 3 / 0 4 / 0 5 / 0 6 / 0 7 / 0 8 / 0 9 / 0 10 / 0 11 / 0 12 / 0 13 / 0 14 / 0 15 / 0 16 / 0 17 / 0 18 / 0 19 / 0 20 / (1, 0.20999999999999996, 'euclidean', 0.8448979591836735)\n",
      "0 21 / (1, 0.21999999999999997, 'euclidean', 0.8483606557377049)\n",
      "0 22 / (1, 0.22999999999999998, 'euclidean', 0.8493723849372385)\n",
      "0 23 / 0 24 / 0 25 / 0 26 / (1, 0.26999999999999996, 'euclidean', 0.8602620087336245)\n",
      "0 27 / (1, 0.27999999999999997, 'euclidean', 0.8616071428571429)\n",
      "0 28 / 0 29 / 1 0 / 1 1 / 1 2 / 1 3 / 1 4 / 1 5 / 1 6 / 1 7 / 1 8 / 1 9 / 1 10 / 1 11 / 1 12 / 1 13 / 1 14 / 1 15 / 1 16 / 1 17 / 1 18 / 1 19 / 1 20 / 1 21 / (2, 0.21999999999999997, 'euclidean', 0.8619246861924686)\n",
      "1 22 / 1 23 / 1 24 / 1 25 / 1 26 / 1 27 / 1 28 / 1 29 / 2 0 / 2 1 / 2 2 / 2 3 / 2 4 / 2 5 / 2 6 / 2 7 / 2 8 / 2 9 / 2 10 / 2 11 / 2 12 / 2 13 / 2 14 / 2 15 / 2 16 / 2 17 / 2 18 / 2 19 / 2 20 / 2 21 / 2 22 / 2 23 / 2 24 / 2 25 / 2 26 / 2 27 / 2 28 / 2 29 / 3 0 / 3 1 / 3 2 / 3 3 / 3 4 / 3 5 / 3 6 / 3 7 / 3 8 / 3 9 / 3 10 / 3 11 / 3 12 / 3 13 / 3 14 / 3 15 / 3 16 / 3 17 / 3 18 / 3 19 / 3 20 / 3 21 / 3 22 / 3 23 / 3 24 / 3 25 / 3 26 / 3 27 / 3 28 / 3 29 / 4 0 / 4 1 / 4 2 / 4 3 / 4 4 / 4 5 / 4 6 / 4 7 / 4 8 / 4 9 / 4 10 / 4 11 / 4 12 / 4 13 / 4 14 / 4 15 / 4 16 / 4 17 / 4 18 / 4 19 / 4 20 / 4 21 / 4 22 / 4 23 / 4 24 / 4 25 / 4 26 / 4 27 / 4 28 / 4 29 / 5 0 / 5 1 / 5 2 / 5 3 / 5 4 / 5 5 / 5 6 / 5 7 / 5 8 / 5 9 / 5 10 / 5 11 / 5 12 / 5 13 / 5 14 / 5 15 / 5 16 / 5 17 / 5 18 / 5 19 / 5 20 / 5 21 / 5 22 / 5 23 / 5 24 / 5 25 / 5 26 / 5 27 / 5 28 / 5 29 / 6 0 / 6 1 / 6 2 / 6 3 / 6 4 / 6 5 / 6 6 / 6 7 / 6 8 / 6 9 / 6 10 / 6 11 / 6 12 / 6 13 / 6 14 / 6 15 / 6 16 / 6 17 / 6 18 / 6 19 / 6 20 / 6 21 / 6 22 / 6 23 / 6 24 / 6 25 / 6 26 / 6 27 / 6 28 / 6 29 / 7 0 / 7 1 / 7 2 / 7 3 / 7 4 / 7 5 / 7 6 / 7 7 / 7 8 / 7 9 / 7 10 / 7 11 / 7 12 / 7 13 / 7 14 / 7 15 / 7 16 / 7 17 / 7 18 / 7 19 / 7 20 / 7 21 / 7 22 / 7 23 / 7 24 / 7 25 / 7 26 / 7 27 / 7 28 / 7 29 / 8 0 / 8 1 / 8 2 / 8 3 / 8 4 / 8 5 / 8 6 / 8 7 / 8 8 / 8 9 / (9, 0.09999999999999998, 'cosine', 0.8623188405797102)\n",
      "8 10 / 8 11 / 8 12 / 8 13 / 8 14 / 8 15 / 8 16 / 8 17 / 8 18 / 8 19 / 8 20 / 8 21 / 8 22 / 8 23 / 8 24 / 8 25 / 8 26 / 8 27 / 8 28 / 8 29 / 9 0 / 9 1 / 9 2 / 9 3 / 9 4 / 9 5 / 9 6 / 9 7 / 9 8 / 9 9 / 9 10 / 9 11 / 9 12 / 9 13 / 9 14 / 9 15 / 9 16 / 9 17 / 9 18 / 9 19 / 9 20 / 9 21 / 9 22 / 9 23 / 9 24 / 9 25 / 9 26 / 9 27 / 9 28 / 9 29 / 10 0 / 10 1 / 10 2 / 10 3 / 10 4 / 10 5 / 10 6 / 10 7 / 10 8 / 10 9 / 10 10 / 10 11 / 10 12 / 10 13 / 10 14 / 10 15 / 10 16 / 10 17 / 10 18 / 10 19 / 10 20 / 10 21 / 10 22 / 10 23 / 10 24 / 10 25 / 10 26 / 10 27 / 10 28 / 10 29 / 11 0 / 11 1 / "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-c542d843c933>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                               \u001b[0mpoly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_poly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                               \u001b[0mrfe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_rfe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                              preset=True)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mxgb_lof_tune\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_lof_tune\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-adb350813731>\u001b[0m in \u001b[0;36mtune_lof_xgb2\u001b[0;34m(model, df, stoprounds, scaler, poly, dim_reduction, rfe, preset)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 mod.fit(X_train_copy, y_train_copy, early_stopping_rounds=stoprounds, eval_metric=\"error\",\n\u001b[0;32m---> 84\u001b[0;31m                      eval_set=[(X_train_copy, y_train_copy), (X_valid_copy, y_valid_copy)], verbose=0)\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;31m# 테스트 정확도 측정 및 최고기록 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/AI_dev/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/AI_dev/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    913\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/AI_dev/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    233\u001b[0m                           \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                           \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                           early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/AI_dev/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mnboost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# check evaluation result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# do checkpoint after evaluation, in case evaluation also updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/AI_dev/lib/python3.7/site-packages/xgboost/callback.py\u001b[0m in \u001b[0;36mafter_iteration\u001b[0;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dataset name should not contain `-`'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# into datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;31m# split up `test-error:0.1234`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/AI_dev/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36meval_set\u001b[0;34m(self, evals, iteration, feval)\u001b[0m\n\u001b[1;32m   1346\u001b[0m                                               \u001b[0mdmats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m                                               \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m                                               ctypes.byref(msg)))\n\u001b[0m\u001b[1;32m   1349\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgb_best = grid7.best_params_['classifier']\n",
    "xgb_scaler = MinMaxScaler()\n",
    "xgb_poly = PolynomialFeatures(degree=3)\n",
    "xgb_rfe = RFE(XGBClassifier(objective='binary:logistic',\n",
    "                           eval_metric='error'),\n",
    "                          n_features_to_select=70)\n",
    "\n",
    "df = (X_train.copy(), X_valid.copy(), X_test.copy(),\n",
    "      y_train.copy(), y_valid.copy(), y_test.copy())\n",
    "\n",
    "xgb_lof_tune = tune_lof_xgb2(xgb_best, df,\n",
    "                              stoprounds=best_esr_stoprounds_rfe[0],\n",
    "                              scaler=xgb_scaler,\n",
    "                              poly=xgb_poly,\n",
    "                              rfe=xgb_rfe,\n",
    "                             preset=True)\n",
    "\n",
    "xgb_lof_tune['best_params'], xgb_lof_tune['best_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-beauty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-ground",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
