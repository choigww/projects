{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "matched-standing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_dev/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn. neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from scipy.stats import expon, reciprocal\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from missingpy import MissForest\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "measured-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(props):\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if props[col].dtype != object:  # Exclude strings\n",
    "            \n",
    "            # Print current column type\n",
    "            #print(\"******************************\")\n",
    "            #print(\"Column: \",col)\n",
    "            #print(\"dtype before: \",props[col].dtype)\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = (props[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "            \n",
    "            # Print new column type\n",
    "            #print(\"dtype after: \",props[col].dtype)\n",
    "            #print(\"******************************\")\n",
    "    \n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return props, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continent-moore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 0.16994476318359375  MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  0.041069984436035156  MB\n",
      "This is  24.166666666666668 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "df, NAli = reduce_mem_usage(pd.read_csv('./data/galaxy_final.csv', index_col=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outer-costa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1485 entries, 0 to 1484\n",
      "Data columns (total 14 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   index                      1485 non-null   uint16 \n",
      " 1   BuyItNow                   1485 non-null   uint8  \n",
      " 2   startprice                 1485 non-null   float32\n",
      " 3   productSeries_imputed      1485 non-null   uint8  \n",
      " 4   product_isNote_imputed     1485 non-null   uint8  \n",
      " 5   hasDescription             1485 non-null   uint8  \n",
      " 6   charCountDescriptionBins   1485 non-null   uint8  \n",
      " 7   upperCaseDescription_rate  1485 non-null   float32\n",
      " 8   startprice_point9          1485 non-null   uint8  \n",
      " 9   sold                       1485 non-null   uint8  \n",
      " 10  color_sentiment_0          1485 non-null   uint8  \n",
      " 11  color_sentiment_1          1485 non-null   uint8  \n",
      " 12  carrier_none_0             1485 non-null   uint8  \n",
      " 13  carrier_none_1             1485 non-null   uint8  \n",
      "dtypes: float32(2), uint16(1), uint8(11)\n",
      "memory usage: 42.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "complete-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('sold', axis=1)\n",
    "y = df.sold\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11,\n",
    "                                                       stratify=y, shuffle=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=11,\n",
    "                                                       stratify=y_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "important-action",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 33.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(base_score=None, booster='dart', colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8, eval_metric='error',\n",
      "              gamma=0, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.1,\n",
      "              max_delta_step=None, max_depth=5, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "              nthread=-1, num_parallel_tree=None, objective='binary:logistic',\n",
      "              random_state=None, rate_drop=0.15, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=1, seed=2021, skip_drop=0.33,\n",
      "              subsample=0.8, tree_method=None, ...), 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__min_child_weight': 1, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.15, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.33, 'classifier__subsample': 0.8, 'feature_selection': RFE(estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                            colsample_bylevel=None, colsample_bynode=None,\n",
      "                            colsample_bytree=None, eval_metric='error',\n",
      "                            gamma=None, gpu_id=None, importance_type='gain',\n",
      "                            interaction_constraints=None, learning_rate=None,\n",
      "                            max_delta_step=None, max_depth=None,\n",
      "                            min_child_weight=None, missing=nan,\n",
      "                            monotone_constraints=None, n_estimators=100,\n",
      "                            n_jobs=None, num_parallel_tree=None,\n",
      "                            objective='binary:logistic', random_state=None,\n",
      "                            reg_alpha=None, reg_lambda=None,\n",
      "                            scale_pos_weight=None, subsample=None,\n",
      "                            tree_method=None, use_label_encoder=True,\n",
      "                            validate_parameters=None, verbosity=None),\n",
      "    n_features_to_select=70, step=1, verbose=0), 'feature_selection__n_features_to_select': 70, 'poly': PolynomialFeatures(degree=3, include_bias=True, interaction_only=False,\n",
      "                   order='C'), 'poly__degree': 3, 'scale': MinMaxScaler(copy=True, feature_range=(0, 1))}\n",
      "0.7477836879432623\n"
     ]
    }
   ],
   "source": [
    "pipe1 = Pipeline([\n",
    "                ('scale', MinMaxScaler()),\n",
    "                 ('poly', PolynomialFeatures()),\n",
    "                ('feature_selection', RFE(XGBClassifier())),\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid1 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': [0.15],\n",
    "              'classifier__skip_drop': [0.33],\n",
    "             'classifier__learning_rate':[0.1],\n",
    "             'classifier__n_estimators':[1000],\n",
    "               'classifier__max_depth':[5],\n",
    "               'classifier__min_child_weight':[1],\n",
    "               'classifier__gamma':[0],\n",
    "               'classifier__subsample':[0.8],\n",
    "               'classifier__colsample_bytree':[0.8],\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1],\n",
    "               'scale':[MinMaxScaler()],\n",
    "               'poly':[PolynomialFeatures()],\n",
    "               'poly__degree': [3],\n",
    "               'feature_selection' : [RFE(XGBClassifier(objective='binary:logistic',\n",
    "                                                       eval_metric='error'))],\n",
    "               'feature_selection__n_features_to_select' : [140, 70, 35, 18]\n",
    "#                'reduce_dims' : [PCA(), LDA(), TSNE()],\n",
    "#                'reduce_dims__n_components' : [5, 7, 9, 11]\n",
    "              }\n",
    "             ]\n",
    "grid1 = GridSearchCV(pipe1, param_grid1, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid1.fit(X_valid, y_valid)\n",
    "print(grid1.best_params_)\n",
    "print(grid1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "integral-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "rfe = RFE(XGBClassifier(objective='binary:logistic', eval_metric='error'),\n",
    "          n_features_to_select=70)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_valid = poly.transform(X_valid)\n",
    "X_test = poly.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "foreign-puzzle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 30s, sys: 6.21 s, total: 29min 36s\n",
      "Wall time: 7min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = rfe.fit_transform(X_train, y_train)\n",
    "X_valid = rfe.transform(X_valid)\n",
    "X_test = rfe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hydraulic-rover",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected False, Rank: 491.000\n",
      "Column: 1, Selected True, Rank: 1.000\n",
      "Column: 2, Selected True, Rank: 1.000\n",
      "Column: 3, Selected True, Rank: 1.000\n",
      "Column: 4, Selected True, Rank: 1.000\n",
      "Column: 5, Selected True, Rank: 1.000\n",
      "Column: 6, Selected False, Rank: 82.000\n",
      "Column: 7, Selected False, Rank: 28.000\n",
      "Column: 8, Selected True, Rank: 1.000\n",
      "Column: 9, Selected False, Rank: 49.000\n",
      "Column: 10, Selected False, Rank: 91.000\n",
      "Column: 11, Selected False, Rank: 66.000\n",
      "Column: 12, Selected False, Rank: 94.000\n",
      "Column: 13, Selected False, Rank: 101.000\n",
      "Column: 14, Selected False, Rank: 103.000\n",
      "Column: 15, Selected True, Rank: 1.000\n",
      "Column: 16, Selected True, Rank: 1.000\n",
      "Column: 17, Selected True, Rank: 1.000\n",
      "Column: 18, Selected True, Rank: 1.000\n",
      "Column: 19, Selected False, Rank: 9.000\n",
      "Column: 20, Selected False, Rank: 63.000\n",
      "Column: 21, Selected True, Rank: 1.000\n",
      "Column: 22, Selected False, Rank: 30.000\n",
      "Column: 23, Selected False, Rank: 17.000\n",
      "Column: 24, Selected True, Rank: 1.000\n",
      "Column: 25, Selected False, Rank: 22.000\n",
      "Column: 26, Selected True, Rank: 1.000\n",
      "Column: 27, Selected False, Rank: 116.000\n",
      "Column: 28, Selected False, Rank: 25.000\n",
      "Column: 29, Selected False, Rank: 23.000\n",
      "Column: 30, Selected False, Rank: 118.000\n",
      "Column: 31, Selected False, Rank: 99.000\n",
      "Column: 32, Selected False, Rank: 126.000\n",
      "Column: 33, Selected False, Rank: 84.000\n",
      "Column: 34, Selected False, Rank: 114.000\n",
      "Column: 35, Selected False, Rank: 142.000\n",
      "Column: 36, Selected False, Rank: 14.000\n",
      "Column: 37, Selected False, Rank: 135.000\n",
      "Column: 38, Selected False, Rank: 76.000\n",
      "Column: 39, Selected False, Rank: 110.000\n",
      "Column: 40, Selected True, Rank: 1.000\n",
      "Column: 41, Selected True, Rank: 1.000\n",
      "Column: 42, Selected True, Rank: 1.000\n",
      "Column: 43, Selected True, Rank: 1.000\n",
      "Column: 44, Selected True, Rank: 1.000\n",
      "Column: 45, Selected True, Rank: 1.000\n",
      "Column: 46, Selected True, Rank: 1.000\n",
      "Column: 47, Selected True, Rank: 1.000\n",
      "Column: 48, Selected True, Rank: 1.000\n",
      "Column: 49, Selected True, Rank: 1.000\n",
      "Column: 50, Selected False, Rank: 140.000\n",
      "Column: 51, Selected False, Rank: 139.000\n",
      "Column: 52, Selected False, Rank: 128.000\n",
      "Column: 53, Selected True, Rank: 1.000\n",
      "Column: 54, Selected True, Rank: 1.000\n",
      "Column: 55, Selected False, Rank: 111.000\n",
      "Column: 56, Selected False, Rank: 148.000\n",
      "Column: 57, Selected False, Rank: 83.000\n",
      "Column: 58, Selected False, Rank: 147.000\n",
      "Column: 59, Selected False, Rank: 8.000\n",
      "Column: 60, Selected False, Rank: 124.000\n",
      "Column: 61, Selected False, Rank: 155.000\n",
      "Column: 62, Selected False, Rank: 117.000\n",
      "Column: 63, Selected False, Rank: 45.000\n",
      "Column: 64, Selected False, Rank: 158.000\n",
      "Column: 65, Selected False, Rank: 160.000\n",
      "Column: 66, Selected False, Rank: 165.000\n",
      "Column: 67, Selected False, Rank: 162.000\n",
      "Column: 68, Selected False, Rank: 164.000\n",
      "Column: 69, Selected False, Rank: 141.000\n",
      "Column: 70, Selected False, Rank: 115.000\n",
      "Column: 71, Selected False, Rank: 149.000\n",
      "Column: 72, Selected False, Rank: 150.000\n",
      "Column: 73, Selected False, Rank: 200.000\n",
      "Column: 74, Selected False, Rank: 132.000\n",
      "Column: 75, Selected False, Rank: 204.000\n",
      "Column: 76, Selected False, Rank: 87.000\n",
      "Column: 77, Selected False, Rank: 208.000\n",
      "Column: 78, Selected False, Rank: 2.000\n",
      "Column: 79, Selected False, Rank: 146.000\n",
      "Column: 80, Selected False, Rank: 145.000\n",
      "Column: 81, Selected False, Rank: 186.000\n",
      "Column: 82, Selected False, Rank: 201.000\n",
      "Column: 83, Selected False, Rank: 205.000\n",
      "Column: 84, Selected False, Rank: 244.000\n",
      "Column: 85, Selected True, Rank: 1.000\n",
      "Column: 86, Selected False, Rank: 190.000\n",
      "Column: 87, Selected False, Rank: 40.000\n",
      "Column: 88, Selected False, Rank: 42.000\n",
      "Column: 89, Selected False, Rank: 74.000\n",
      "Column: 90, Selected False, Rank: 212.000\n",
      "Column: 91, Selected False, Rank: 214.000\n",
      "Column: 92, Selected False, Rank: 216.000\n",
      "Column: 93, Selected False, Rank: 218.000\n",
      "Column: 94, Selected False, Rank: 166.000\n",
      "Column: 95, Selected False, Rank: 168.000\n",
      "Column: 96, Selected False, Rank: 170.000\n",
      "Column: 97, Selected False, Rank: 172.000\n",
      "Column: 98, Selected False, Rank: 174.000\n",
      "Column: 99, Selected False, Rank: 176.000\n",
      "Column: 100, Selected False, Rank: 178.000\n",
      "Column: 101, Selected True, Rank: 1.000\n",
      "Column: 102, Selected False, Rank: 181.000\n",
      "Column: 103, Selected False, Rank: 183.000\n",
      "Column: 104, Selected False, Rank: 187.000\n",
      "Column: 105, Selected False, Rank: 191.000\n",
      "Column: 106, Selected False, Rank: 193.000\n",
      "Column: 107, Selected True, Rank: 1.000\n",
      "Column: 108, Selected True, Rank: 1.000\n",
      "Column: 109, Selected False, Rank: 263.000\n",
      "Column: 110, Selected False, Rank: 265.000\n",
      "Column: 111, Selected False, Rank: 267.000\n",
      "Column: 112, Selected True, Rank: 1.000\n",
      "Column: 113, Selected False, Rank: 272.000\n",
      "Column: 114, Selected False, Rank: 273.000\n",
      "Column: 115, Selected False, Rank: 275.000\n",
      "Column: 116, Selected False, Rank: 258.000\n",
      "Column: 117, Selected False, Rank: 219.000\n",
      "Column: 118, Selected False, Rank: 221.000\n",
      "Column: 119, Selected False, Rank: 39.000\n",
      "Column: 120, Selected True, Rank: 1.000\n",
      "Column: 121, Selected False, Rank: 68.000\n",
      "Column: 122, Selected False, Rank: 43.000\n",
      "Column: 123, Selected False, Rank: 16.000\n",
      "Column: 124, Selected False, Rank: 268.000\n",
      "Column: 125, Selected False, Rank: 58.000\n",
      "Column: 126, Selected False, Rank: 236.000\n",
      "Column: 127, Selected False, Rank: 27.000\n",
      "Column: 128, Selected False, Rank: 240.000\n",
      "Column: 129, Selected False, Rank: 55.000\n",
      "Column: 130, Selected True, Rank: 1.000\n",
      "Column: 131, Selected True, Rank: 1.000\n",
      "Column: 132, Selected True, Rank: 1.000\n",
      "Column: 133, Selected True, Rank: 1.000\n",
      "Column: 134, Selected False, Rank: 95.000\n",
      "Column: 135, Selected False, Rank: 19.000\n",
      "Column: 136, Selected False, Rank: 7.000\n",
      "Column: 137, Selected True, Rank: 1.000\n",
      "Column: 138, Selected False, Rank: 36.000\n",
      "Column: 139, Selected False, Rank: 47.000\n",
      "Column: 140, Selected False, Rank: 54.000\n",
      "Column: 141, Selected True, Rank: 1.000\n",
      "Column: 142, Selected True, Rank: 1.000\n",
      "Column: 143, Selected True, Rank: 1.000\n",
      "Column: 144, Selected False, Rank: 21.000\n",
      "Column: 145, Selected True, Rank: 1.000\n",
      "Column: 146, Selected False, Rank: 10.000\n",
      "Column: 147, Selected True, Rank: 1.000\n",
      "Column: 148, Selected True, Rank: 1.000\n",
      "Column: 149, Selected True, Rank: 1.000\n",
      "Column: 150, Selected False, Rank: 24.000\n",
      "Column: 151, Selected False, Rank: 304.000\n",
      "Column: 152, Selected False, Rank: 35.000\n",
      "Column: 153, Selected False, Rank: 308.000\n",
      "Column: 154, Selected False, Rank: 5.000\n",
      "Column: 155, Selected False, Rank: 48.000\n",
      "Column: 156, Selected True, Rank: 1.000\n",
      "Column: 157, Selected False, Rank: 316.000\n",
      "Column: 158, Selected True, Rank: 1.000\n",
      "Column: 159, Selected False, Rank: 61.000\n",
      "Column: 160, Selected False, Rank: 322.000\n",
      "Column: 161, Selected False, Rank: 324.000\n",
      "Column: 162, Selected False, Rank: 326.000\n",
      "Column: 163, Selected False, Rank: 328.000\n",
      "Column: 164, Selected False, Rank: 330.000\n",
      "Column: 165, Selected False, Rank: 332.000\n",
      "Column: 166, Selected False, Rank: 334.000\n",
      "Column: 167, Selected False, Rank: 4.000\n",
      "Column: 168, Selected False, Rank: 225.000\n",
      "Column: 169, Selected True, Rank: 1.000\n",
      "Column: 170, Selected True, Rank: 1.000\n",
      "Column: 171, Selected False, Rank: 338.000\n",
      "Column: 172, Selected False, Rank: 340.000\n",
      "Column: 173, Selected False, Rank: 342.000\n",
      "Column: 174, Selected False, Rank: 38.000\n",
      "Column: 175, Selected False, Rank: 75.000\n",
      "Column: 176, Selected False, Rank: 37.000\n",
      "Column: 177, Selected False, Rank: 81.000\n",
      "Column: 178, Selected False, Rank: 65.000\n",
      "Column: 179, Selected False, Rank: 56.000\n",
      "Column: 180, Selected False, Rank: 18.000\n",
      "Column: 181, Selected False, Rank: 364.000\n",
      "Column: 182, Selected False, Rank: 29.000\n",
      "Column: 183, Selected False, Rank: 368.000\n",
      "Column: 184, Selected True, Rank: 1.000\n",
      "Column: 185, Selected False, Rank: 52.000\n",
      "Column: 186, Selected False, Rank: 374.000\n",
      "Column: 187, Selected False, Rank: 376.000\n",
      "Column: 188, Selected False, Rank: 378.000\n",
      "Column: 189, Selected False, Rank: 73.000\n",
      "Column: 190, Selected False, Rank: 382.000\n",
      "Column: 191, Selected False, Rank: 384.000\n",
      "Column: 192, Selected True, Rank: 1.000\n",
      "Column: 193, Selected False, Rank: 285.000\n",
      "Column: 194, Selected False, Rank: 389.000\n",
      "Column: 195, Selected False, Rank: 391.000\n",
      "Column: 196, Selected False, Rank: 393.000\n",
      "Column: 197, Selected False, Rank: 395.000\n",
      "Column: 198, Selected False, Rank: 398.000\n",
      "Column: 199, Selected False, Rank: 400.000\n",
      "Column: 200, Selected False, Rank: 402.000\n",
      "Column: 201, Selected False, Rank: 401.000\n",
      "Column: 202, Selected False, Rank: 399.000\n",
      "Column: 203, Selected False, Rank: 397.000\n",
      "Column: 204, Selected False, Rank: 287.000\n",
      "Column: 205, Selected False, Rank: 277.000\n",
      "Column: 206, Selected False, Rank: 388.000\n",
      "Column: 207, Selected False, Rank: 387.000\n",
      "Column: 208, Selected False, Rank: 279.000\n",
      "Column: 209, Selected False, Rank: 32.000\n",
      "Column: 210, Selected True, Rank: 1.000\n",
      "Column: 211, Selected False, Rank: 6.000\n",
      "Column: 212, Selected False, Rank: 64.000\n",
      "Column: 213, Selected False, Rank: 346.000\n",
      "Column: 214, Selected False, Rank: 348.000\n",
      "Column: 215, Selected False, Rank: 350.000\n",
      "Column: 216, Selected False, Rank: 352.000\n",
      "Column: 217, Selected False, Rank: 354.000\n",
      "Column: 218, Selected False, Rank: 46.000\n",
      "Column: 219, Selected False, Rank: 357.000\n",
      "Column: 220, Selected False, Rank: 359.000\n",
      "Column: 221, Selected False, Rank: 361.000\n",
      "Column: 222, Selected False, Rank: 365.000\n",
      "Column: 223, Selected False, Rank: 369.000\n",
      "Column: 224, Selected False, Rank: 371.000\n",
      "Column: 225, Selected False, Rank: 379.000\n",
      "Column: 226, Selected False, Rank: 386.000\n",
      "Column: 227, Selected False, Rank: 385.000\n",
      "Column: 228, Selected False, Rank: 70.000\n",
      "Column: 229, Selected False, Rank: 297.000\n",
      "Column: 230, Selected False, Rank: 299.000\n",
      "Column: 231, Selected False, Rank: 301.000\n",
      "Column: 232, Selected False, Rank: 44.000\n",
      "Column: 233, Selected False, Rank: 309.000\n",
      "Column: 234, Selected False, Rank: 311.000\n",
      "Column: 235, Selected False, Rank: 313.000\n",
      "Column: 236, Selected False, Rank: 403.000\n",
      "Column: 237, Selected False, Rank: 405.000\n",
      "Column: 238, Selected False, Rank: 407.000\n",
      "Column: 239, Selected False, Rank: 409.000\n",
      "Column: 240, Selected False, Rank: 411.000\n",
      "Column: 241, Selected False, Rank: 413.000\n",
      "Column: 242, Selected False, Rank: 415.000\n",
      "Column: 243, Selected False, Rank: 417.000\n",
      "Column: 244, Selected False, Rank: 419.000\n",
      "Column: 245, Selected False, Rank: 421.000\n",
      "Column: 246, Selected False, Rank: 423.000\n",
      "Column: 247, Selected False, Rank: 425.000\n",
      "Column: 248, Selected False, Rank: 427.000\n",
      "Column: 249, Selected False, Rank: 429.000\n",
      "Column: 250, Selected False, Rank: 431.000\n",
      "Column: 251, Selected False, Rank: 433.000\n",
      "Column: 252, Selected False, Rank: 435.000\n",
      "Column: 253, Selected False, Rank: 437.000\n",
      "Column: 254, Selected False, Rank: 439.000\n",
      "Column: 255, Selected False, Rank: 441.000\n",
      "Column: 256, Selected False, Rank: 443.000\n",
      "Column: 257, Selected False, Rank: 445.000\n",
      "Column: 258, Selected False, Rank: 72.000\n",
      "Column: 259, Selected False, Rank: 449.000\n",
      "Column: 260, Selected False, Rank: 451.000\n",
      "Column: 261, Selected False, Rank: 453.000\n",
      "Column: 262, Selected False, Rank: 455.000\n",
      "Column: 263, Selected False, Rank: 457.000\n",
      "Column: 264, Selected False, Rank: 459.000\n",
      "Column: 265, Selected False, Rank: 461.000\n",
      "Column: 266, Selected False, Rank: 463.000\n",
      "Column: 267, Selected False, Rank: 465.000\n",
      "Column: 268, Selected False, Rank: 467.000\n",
      "Column: 269, Selected False, Rank: 469.000\n",
      "Column: 270, Selected False, Rank: 471.000\n",
      "Column: 271, Selected False, Rank: 473.000\n",
      "Column: 272, Selected False, Rank: 475.000\n",
      "Column: 273, Selected False, Rank: 477.000\n",
      "Column: 274, Selected False, Rank: 479.000\n",
      "Column: 275, Selected True, Rank: 1.000\n",
      "Column: 276, Selected False, Rank: 483.000\n",
      "Column: 277, Selected False, Rank: 485.000\n",
      "Column: 278, Selected False, Rank: 487.000\n",
      "Column: 279, Selected True, Rank: 1.000\n",
      "Column: 280, Selected False, Rank: 490.000\n",
      "Column: 281, Selected False, Rank: 396.000\n",
      "Column: 282, Selected False, Rank: 394.000\n",
      "Column: 283, Selected False, Rank: 392.000\n",
      "Column: 284, Selected False, Rank: 390.000\n",
      "Column: 285, Selected True, Rank: 1.000\n",
      "Column: 286, Selected True, Rank: 1.000\n",
      "Column: 287, Selected True, Rank: 1.000\n",
      "Column: 288, Selected False, Rank: 57.000\n",
      "Column: 289, Selected False, Rank: 20.000\n",
      "Column: 290, Selected False, Rank: 15.000\n",
      "Column: 291, Selected False, Rank: 69.000\n",
      "Column: 292, Selected False, Rank: 80.000\n",
      "Column: 293, Selected True, Rank: 1.000\n",
      "Column: 294, Selected False, Rank: 11.000\n",
      "Column: 295, Selected False, Rank: 152.000\n",
      "Column: 296, Selected False, Rank: 210.000\n",
      "Column: 297, Selected False, Rank: 153.000\n",
      "Column: 298, Selected False, Rank: 78.000\n",
      "Column: 299, Selected False, Rank: 41.000\n",
      "Column: 300, Selected False, Rank: 3.000\n",
      "Column: 301, Selected False, Rank: 137.000\n",
      "Column: 302, Selected False, Rank: 26.000\n",
      "Column: 303, Selected False, Rank: 77.000\n",
      "Column: 304, Selected False, Rank: 167.000\n",
      "Column: 305, Selected False, Rank: 169.000\n",
      "Column: 306, Selected False, Rank: 171.000\n",
      "Column: 307, Selected False, Rank: 173.000\n",
      "Column: 308, Selected False, Rank: 31.000\n",
      "Column: 309, Selected False, Rank: 177.000\n",
      "Column: 310, Selected False, Rank: 59.000\n",
      "Column: 311, Selected True, Rank: 1.000\n",
      "Column: 312, Selected False, Rank: 194.000\n",
      "Column: 313, Selected True, Rank: 1.000\n",
      "Column: 314, Selected False, Rank: 179.000\n",
      "Column: 315, Selected False, Rank: 175.000\n",
      "Column: 316, Selected True, Rank: 1.000\n",
      "Column: 317, Selected False, Rank: 182.000\n",
      "Column: 318, Selected False, Rank: 184.000\n",
      "Column: 319, Selected False, Rank: 60.000\n",
      "Column: 320, Selected False, Rank: 34.000\n",
      "Column: 321, Selected False, Rank: 197.000\n",
      "Column: 322, Selected False, Rank: 198.000\n",
      "Column: 323, Selected False, Rank: 67.000\n",
      "Column: 324, Selected False, Rank: 33.000\n",
      "Column: 325, Selected False, Rank: 261.000\n",
      "Column: 326, Selected True, Rank: 1.000\n",
      "Column: 327, Selected False, Rank: 206.000\n",
      "Column: 328, Selected True, Rank: 1.000\n",
      "Column: 329, Selected True, Rank: 1.000\n",
      "Column: 330, Selected False, Rank: 211.000\n",
      "Column: 331, Selected False, Rank: 213.000\n",
      "Column: 332, Selected False, Rank: 215.000\n",
      "Column: 333, Selected True, Rank: 1.000\n",
      "Column: 334, Selected False, Rank: 217.000\n",
      "Column: 335, Selected False, Rank: 79.000\n",
      "Column: 336, Selected False, Rank: 151.000\n",
      "Column: 337, Selected False, Rank: 120.000\n",
      "Column: 338, Selected False, Rank: 156.000\n",
      "Column: 339, Selected False, Rank: 104.000\n",
      "Column: 340, Selected False, Rank: 113.000\n",
      "Column: 341, Selected False, Rank: 136.000\n",
      "Column: 342, Selected False, Rank: 154.000\n",
      "Column: 343, Selected False, Rank: 134.000\n",
      "Column: 344, Selected False, Rank: 85.000\n",
      "Column: 345, Selected False, Rank: 233.000\n",
      "Column: 346, Selected False, Rank: 234.000\n",
      "Column: 347, Selected False, Rank: 235.000\n",
      "Column: 348, Selected False, Rank: 237.000\n",
      "Column: 349, Selected False, Rank: 238.000\n",
      "Column: 350, Selected False, Rank: 239.000\n",
      "Column: 351, Selected False, Rank: 241.000\n",
      "Column: 352, Selected False, Rank: 242.000\n",
      "Column: 353, Selected False, Rank: 71.000\n",
      "Column: 354, Selected False, Rank: 92.000\n",
      "Column: 355, Selected False, Rank: 138.000\n",
      "Column: 356, Selected False, Rank: 255.000\n",
      "Column: 357, Selected False, Rank: 247.000\n",
      "Column: 358, Selected False, Rank: 262.000\n",
      "Column: 359, Selected False, Rank: 264.000\n",
      "Column: 360, Selected False, Rank: 266.000\n",
      "Column: 361, Selected False, Rank: 123.000\n",
      "Column: 362, Selected False, Rank: 125.000\n",
      "Column: 363, Selected False, Rank: 127.000\n",
      "Column: 364, Selected False, Rank: 51.000\n",
      "Column: 365, Selected False, Rank: 251.000\n",
      "Column: 366, Selected False, Rank: 129.000\n",
      "Column: 367, Selected False, Rank: 131.000\n",
      "Column: 368, Selected True, Rank: 1.000\n",
      "Column: 369, Selected True, Rank: 1.000\n",
      "Column: 370, Selected False, Rank: 259.000\n",
      "Column: 371, Selected True, Rank: 1.000\n",
      "Column: 372, Selected False, Rank: 269.000\n",
      "Column: 373, Selected False, Rank: 270.000\n",
      "Column: 374, Selected False, Rank: 12.000\n",
      "Column: 375, Selected True, Rank: 1.000\n",
      "Column: 376, Selected False, Rank: 276.000\n",
      "Column: 377, Selected False, Rank: 50.000\n",
      "Column: 378, Selected True, Rank: 1.000\n",
      "Column: 379, Selected False, Rank: 281.000\n",
      "Column: 380, Selected False, Rank: 280.000\n",
      "Column: 381, Selected False, Rank: 248.000\n",
      "Column: 382, Selected False, Rank: 157.000\n",
      "Column: 383, Selected False, Rank: 274.000\n",
      "Column: 384, Selected False, Rank: 159.000\n",
      "Column: 385, Selected False, Rank: 161.000\n",
      "Column: 386, Selected False, Rank: 290.000\n",
      "Column: 387, Selected False, Rank: 163.000\n",
      "Column: 388, Selected False, Rank: 62.000\n",
      "Column: 389, Selected False, Rank: 293.000\n",
      "Column: 390, Selected False, Rank: 294.000\n",
      "Column: 391, Selected False, Rank: 220.000\n",
      "Column: 392, Selected False, Rank: 222.000\n",
      "Column: 393, Selected False, Rank: 223.000\n",
      "Column: 394, Selected False, Rank: 185.000\n",
      "Column: 395, Selected False, Rank: 302.000\n",
      "Column: 396, Selected False, Rank: 133.000\n",
      "Column: 397, Selected False, Rank: 189.000\n",
      "Column: 398, Selected False, Rank: 143.000\n",
      "Column: 399, Selected False, Rank: 195.000\n",
      "Column: 400, Selected False, Rank: 199.000\n",
      "Column: 401, Selected False, Rank: 312.000\n",
      "Column: 402, Selected False, Rank: 252.000\n",
      "Column: 403, Selected False, Rank: 315.000\n",
      "Column: 404, Selected False, Rank: 317.000\n",
      "Column: 405, Selected False, Rank: 318.000\n",
      "Column: 406, Selected False, Rank: 319.000\n",
      "Column: 407, Selected False, Rank: 320.000\n",
      "Column: 408, Selected False, Rank: 253.000\n",
      "Column: 409, Selected False, Rank: 196.000\n",
      "Column: 410, Selected False, Rank: 203.000\n",
      "Column: 411, Selected False, Rank: 207.000\n",
      "Column: 412, Selected False, Rank: 224.000\n",
      "Column: 413, Selected False, Rank: 226.000\n",
      "Column: 414, Selected False, Rank: 227.000\n",
      "Column: 415, Selected False, Rank: 209.000\n",
      "Column: 416, Selected False, Rank: 112.000\n",
      "Column: 417, Selected False, Rank: 231.000\n",
      "Column: 418, Selected False, Rank: 243.000\n",
      "Column: 419, Selected False, Rank: 245.000\n",
      "Column: 420, Selected False, Rank: 246.000\n",
      "Column: 421, Selected False, Rank: 250.000\n",
      "Column: 422, Selected False, Rank: 232.000\n",
      "Column: 423, Selected False, Rank: 337.000\n",
      "Column: 424, Selected False, Rank: 53.000\n",
      "Column: 425, Selected False, Rank: 341.000\n",
      "Column: 426, Selected False, Rank: 343.000\n",
      "Column: 427, Selected False, Rank: 339.000\n",
      "Column: 428, Selected False, Rank: 356.000\n",
      "Column: 429, Selected False, Rank: 358.000\n",
      "Column: 430, Selected False, Rank: 360.000\n",
      "Column: 431, Selected False, Rank: 230.000\n",
      "Column: 432, Selected False, Rank: 363.000\n",
      "Column: 433, Selected False, Rank: 366.000\n",
      "Column: 434, Selected False, Rank: 192.000\n",
      "Column: 435, Selected False, Rank: 271.000\n",
      "Column: 436, Selected False, Rank: 278.000\n",
      "Column: 437, Selected False, Rank: 373.000\n",
      "Column: 438, Selected False, Rank: 375.000\n",
      "Column: 439, Selected False, Rank: 377.000\n",
      "Column: 440, Selected False, Rank: 380.000\n",
      "Column: 441, Selected False, Rank: 381.000\n",
      "Column: 442, Selected False, Rank: 383.000\n",
      "Column: 443, Selected False, Rank: 254.000\n",
      "Column: 444, Selected False, Rank: 257.000\n",
      "Column: 445, Selected False, Rank: 260.000\n",
      "Column: 446, Selected False, Rank: 291.000\n",
      "Column: 447, Selected False, Rank: 295.000\n",
      "Column: 448, Selected False, Rank: 303.000\n",
      "Column: 449, Selected False, Rank: 305.000\n",
      "Column: 450, Selected False, Rank: 307.000\n",
      "Column: 451, Selected False, Rank: 321.000\n",
      "Column: 452, Selected False, Rank: 323.000\n",
      "Column: 453, Selected False, Rank: 325.000\n",
      "Column: 454, Selected False, Rank: 327.000\n",
      "Column: 455, Selected False, Rank: 329.000\n",
      "Column: 456, Selected False, Rank: 331.000\n",
      "Column: 457, Selected False, Rank: 333.000\n",
      "Column: 458, Selected False, Rank: 335.000\n",
      "Column: 459, Selected False, Rank: 314.000\n",
      "Column: 460, Selected False, Rank: 336.000\n",
      "Column: 461, Selected False, Rank: 310.000\n",
      "Column: 462, Selected False, Rank: 306.000\n",
      "Column: 463, Selected False, Rank: 300.000\n",
      "Column: 464, Selected False, Rank: 367.000\n",
      "Column: 465, Selected False, Rank: 372.000\n",
      "Column: 466, Selected False, Rank: 370.000\n",
      "Column: 467, Selected False, Rank: 362.000\n",
      "Column: 468, Selected False, Rank: 298.000\n",
      "Column: 469, Selected False, Rank: 296.000\n",
      "Column: 470, Selected False, Rank: 292.000\n",
      "Column: 471, Selected False, Rank: 249.000\n",
      "Column: 472, Selected False, Rank: 256.000\n",
      "Column: 473, Selected False, Rank: 188.000\n",
      "Column: 474, Selected False, Rank: 180.000\n",
      "Column: 475, Selected False, Rank: 202.000\n",
      "Column: 476, Selected False, Rank: 130.000\n",
      "Column: 477, Selected False, Rank: 144.000\n",
      "Column: 478, Selected False, Rank: 106.000\n",
      "Column: 479, Selected False, Rank: 107.000\n",
      "Column: 480, Selected False, Rank: 344.000\n",
      "Column: 481, Selected False, Rank: 345.000\n",
      "Column: 482, Selected False, Rank: 347.000\n",
      "Column: 483, Selected False, Rank: 349.000\n",
      "Column: 484, Selected False, Rank: 351.000\n",
      "Column: 485, Selected False, Rank: 353.000\n",
      "Column: 486, Selected False, Rank: 355.000\n",
      "Column: 487, Selected False, Rank: 122.000\n",
      "Column: 488, Selected False, Rank: 13.000\n",
      "Column: 489, Selected False, Rank: 121.000\n",
      "Column: 490, Selected False, Rank: 119.000\n",
      "Column: 491, Selected False, Rank: 102.000\n",
      "Column: 492, Selected False, Rank: 100.000\n",
      "Column: 493, Selected False, Rank: 105.000\n",
      "Column: 494, Selected False, Rank: 284.000\n",
      "Column: 495, Selected False, Rank: 286.000\n",
      "Column: 496, Selected False, Rank: 288.000\n",
      "Column: 497, Selected False, Rank: 289.000\n",
      "Column: 498, Selected False, Rank: 108.000\n",
      "Column: 499, Selected False, Rank: 97.000\n",
      "Column: 500, Selected False, Rank: 98.000\n",
      "Column: 501, Selected False, Rank: 86.000\n",
      "Column: 502, Selected False, Rank: 228.000\n",
      "Column: 503, Selected False, Rank: 229.000\n",
      "Column: 504, Selected False, Rank: 282.000\n",
      "Column: 505, Selected False, Rank: 283.000\n",
      "Column: 506, Selected False, Rank: 88.000\n",
      "Column: 507, Selected False, Rank: 89.000\n",
      "Column: 508, Selected False, Rank: 90.000\n",
      "Column: 509, Selected False, Rank: 93.000\n",
      "Column: 510, Selected False, Rank: 96.000\n",
      "Column: 511, Selected True, Rank: 1.000\n",
      "Column: 512, Selected False, Rank: 109.000\n",
      "Column: 513, Selected True, Rank: 1.000\n",
      "Column: 514, Selected False, Rank: 404.000\n",
      "Column: 515, Selected False, Rank: 406.000\n",
      "Column: 516, Selected False, Rank: 408.000\n",
      "Column: 517, Selected False, Rank: 410.000\n",
      "Column: 518, Selected False, Rank: 412.000\n",
      "Column: 519, Selected False, Rank: 414.000\n",
      "Column: 520, Selected False, Rank: 416.000\n",
      "Column: 521, Selected False, Rank: 418.000\n",
      "Column: 522, Selected False, Rank: 420.000\n",
      "Column: 523, Selected False, Rank: 422.000\n",
      "Column: 524, Selected False, Rank: 424.000\n",
      "Column: 525, Selected False, Rank: 426.000\n",
      "Column: 526, Selected False, Rank: 428.000\n",
      "Column: 527, Selected False, Rank: 430.000\n",
      "Column: 528, Selected False, Rank: 432.000\n",
      "Column: 529, Selected False, Rank: 434.000\n",
      "Column: 530, Selected False, Rank: 436.000\n",
      "Column: 531, Selected False, Rank: 438.000\n",
      "Column: 532, Selected False, Rank: 440.000\n",
      "Column: 533, Selected False, Rank: 442.000\n",
      "Column: 534, Selected False, Rank: 444.000\n",
      "Column: 535, Selected False, Rank: 446.000\n",
      "Column: 536, Selected False, Rank: 447.000\n",
      "Column: 537, Selected False, Rank: 448.000\n",
      "Column: 538, Selected False, Rank: 450.000\n",
      "Column: 539, Selected False, Rank: 452.000\n",
      "Column: 540, Selected False, Rank: 454.000\n",
      "Column: 541, Selected False, Rank: 456.000\n",
      "Column: 542, Selected False, Rank: 458.000\n",
      "Column: 543, Selected False, Rank: 460.000\n",
      "Column: 544, Selected False, Rank: 462.000\n",
      "Column: 545, Selected False, Rank: 464.000\n",
      "Column: 546, Selected False, Rank: 466.000\n",
      "Column: 547, Selected False, Rank: 468.000\n",
      "Column: 548, Selected False, Rank: 470.000\n",
      "Column: 549, Selected False, Rank: 472.000\n",
      "Column: 550, Selected False, Rank: 474.000\n",
      "Column: 551, Selected False, Rank: 476.000\n",
      "Column: 552, Selected False, Rank: 478.000\n",
      "Column: 553, Selected False, Rank: 480.000\n",
      "Column: 554, Selected False, Rank: 481.000\n",
      "Column: 555, Selected False, Rank: 482.000\n",
      "Column: 556, Selected False, Rank: 484.000\n",
      "Column: 557, Selected False, Rank: 486.000\n",
      "Column: 558, Selected False, Rank: 488.000\n",
      "Column: 559, Selected False, Rank: 489.000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 560 is out of bounds for axis 0 with size 560",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-73aa3772fd90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Column: %d, Selected %s, Rank: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mranking_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 560 is out of bounds for axis 0 with size 560"
     ]
    }
   ],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pointed-attention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of columns:70\n"
     ]
    }
   ],
   "source": [
    "print(f'num of columns:{len(X_train[0, :])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "prompt-coast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 105 candidates, totalling 525 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 29.7min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 64.8min\n",
      "[Parallel(n_jobs=-1)]: Done 525 out of 525 | elapsed: 78.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(base_score=None, booster='dart', colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8, eval_metric='error',\n",
      "              gamma=0, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.02,\n",
      "              max_delta_step=None, max_depth=9, min_child_weight=4, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "              nthread=-1, num_parallel_tree=None, objective='binary:logistic',\n",
      "              random_state=None, rate_drop=0.15, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=1, seed=2021, skip_drop=0.33,\n",
      "              subsample=0.8, tree_method=None, ...), 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 9, 'classifier__min_child_weight': 4, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.15, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.33, 'classifier__subsample': 0.8}\n",
      "0.8221052631578948\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid2 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': [0.15],\n",
    "              'classifier__skip_drop': [0.33],\n",
    "             'classifier__learning_rate':[0.01, 0.02, 0.03],\n",
    "             'classifier__n_estimators':[1000],\n",
    "               'classifier__max_depth':range(3,10),\n",
    "               'classifier__min_child_weight':range(1,6),\n",
    "               'classifier__gamma':[0],\n",
    "               'classifier__subsample':[0.8],\n",
    "               'classifier__colsample_bytree':[0.8],\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1],\n",
    "              }\n",
    "             ]\n",
    "grid2 = GridSearchCV(pipe, param_grid2, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid2.fit(X_train, y_train)\n",
    "print(grid2.best_params_)\n",
    "print(grid2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-strip",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "entitled-blind",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  9.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(base_score=None, booster='dart', colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8, eval_metric='error',\n",
      "              gamma=0.0, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.02,\n",
      "              max_delta_step=None, max_depth=9, min_child_weight=4, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "              nthread=-1, num_parallel_tree=None, objective='binary:logistic',\n",
      "              random_state=None, rate_drop=0.15, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=1, seed=2021, skip_drop=0.33,\n",
      "              subsample=0.8, tree_method=None, ...), 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0.0, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 9, 'classifier__min_child_weight': 4, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.15, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.33, 'classifier__subsample': 0.8}\n",
      "0.8221052631578948\n"
     ]
    }
   ],
   "source": [
    "# Gamma \n",
    "\n",
    "pipe = Pipeline([\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid3 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': [0.15],\n",
    "              'classifier__skip_drop': [0.33],\n",
    "             'classifier__learning_rate':[grid2.best_params_['classifier__learning_rate']],\n",
    "             'classifier__n_estimators':[1000],\n",
    "               'classifier__max_depth': [grid2.best_params_['classifier__max_depth']], # 3\n",
    "               'classifier__min_child_weight': [grid2.best_params_['classifier__min_child_weight']],# 3\n",
    "               'classifier__gamma':[i/10.0 for i in range(0,10)],\n",
    "               'classifier__subsample':[0.8],\n",
    "               'classifier__colsample_bytree':[0.8],\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1],\n",
    "#                'scale':[MinMaxScaler()],\n",
    "#                'poly':[PolynomialFeatures()],\n",
    "#                'poly__degree':[3],\n",
    "#                'feature_selection' : [RFE(RandomForestClassifier())],\n",
    "#                'feature_selection__n_features_to_select' : [140]\n",
    "              }\n",
    "             ]\n",
    "grid3 = GridSearchCV(pipe, param_grid3, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid3.fit(X_train, y_train)\n",
    "print(grid3.best_params_)\n",
    "print(grid3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-rates",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "proud-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed: 13.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(base_score=None, booster='dart', colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8, eval_metric='error',\n",
      "              gamma=0.0, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.02,\n",
      "              max_delta_step=None, max_depth=9, min_child_weight=4, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "              nthread=-1, num_parallel_tree=None, objective='binary:logistic',\n",
      "              random_state=None, rate_drop=0.15, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=1, seed=2021, skip_drop=0.33,\n",
      "              subsample=0.8, tree_method=None, ...), 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0.0, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 9, 'classifier__min_child_weight': 4, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.15, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.33, 'classifier__subsample': 0.8}\n",
      "0.8221052631578948\n"
     ]
    }
   ],
   "source": [
    "# subsample and colsample_bytree .\n",
    "\n",
    "pipe= Pipeline([\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid4 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': [0.15],\n",
    "              'classifier__skip_drop': [0.33],\n",
    "             'classifier__learning_rate':[grid2.best_params_['classifier__learning_rate']],\n",
    "             'classifier__n_estimators':[1000],\n",
    "               'classifier__max_depth': [grid2.best_params_['classifier__max_depth']], # 3\n",
    "               'classifier__min_child_weight': [grid2.best_params_['classifier__min_child_weight']],# 3\n",
    "               'classifier__gamma':[grid3.best_params_['classifier__gamma']], # 0.5\n",
    "               'classifier__subsample':[i/10.0 for i in range(6,10)],\n",
    "               'classifier__colsample_bytree':[i/10.0 for i in range(6,10)],\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1]\n",
    "              }\n",
    "             ]\n",
    "grid4 = GridSearchCV(pipe, param_grid4, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid4.fit(X_train, y_train)\n",
    "print(grid4.best_params_)\n",
    "print(grid4.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-somerset",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "naughty-wholesale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(base_score=None, booster='dart', colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8, eval_metric='error',\n",
      "              gamma=0.0, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.02,\n",
      "              max_delta_step=None, max_depth=9, min_child_weight=4, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "              nthread=-1, num_parallel_tree=None, objective='binary:logistic',\n",
      "              random_state=None, rate_drop=0.15, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=1, seed=2021, skip_drop=0.33,\n",
      "              subsample=0.8, tree_method=None, ...), 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0.0, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 9, 'classifier__min_child_weight': 4, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.15, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.33, 'classifier__subsample': 0.8}\n",
      "0.8221052631578948\n"
     ]
    }
   ],
   "source": [
    "# n_estimator \n",
    "\n",
    "pipe = Pipeline([\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid5 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': [0.15],\n",
    "              'classifier__skip_drop': [0.33],\n",
    "             'classifier__learning_rate':[grid2.best_params_['classifier__learning_rate']],\n",
    "             'classifier__n_estimators':[100, 200, 300, 400, 500, 700, 1000],\n",
    "               'classifier__max_depth': [grid2.best_params_['classifier__max_depth']], # 3\n",
    "               'classifier__min_child_weight': [grid2.best_params_['classifier__min_child_weight']],# 3\n",
    "               'classifier__gamma':[grid3.best_params_['classifier__gamma']], # 0.5\n",
    "               'classifier__subsample':[grid4.best_params_['classifier__subsample']], # 0.8\n",
    "               'classifier__colsample_bytree':[grid4.best_params_['classifier__colsample_bytree']], # 0.8\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1],\n",
    "\n",
    "              }\n",
    "             ]\n",
    "grid5 = GridSearchCV(pipe, param_grid5, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid5.fit(X_train, y_train)\n",
    "print(grid5.best_params_)\n",
    "print(grid5.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-vertical",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "closed-money",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 33.8min\n",
      "[Parallel(n_jobs=-1)]: Done 405 out of 405 | elapsed: 66.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(base_score=None, booster='dart', colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8, eval_metric='error',\n",
      "              gamma=0.0, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.02,\n",
      "              max_delta_step=None, max_depth=9, min_child_weight=4, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "              nthread=-1, num_parallel_tree=None, objective='binary:logistic',\n",
      "              random_state=None, rate_drop=0.20000000000000004, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=1, seed=2021,\n",
      "              skip_drop=0.30000000000000004, subsample=0.8, tree_method=None, ...), 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0.0, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 9, 'classifier__min_child_weight': 4, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.20000000000000004, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.30000000000000004, 'classifier__subsample': 0.8}\n",
      "0.8221052631578948\n"
     ]
    }
   ],
   "source": [
    "# rate_drop, skip_drop \n",
    "\n",
    "pipe = Pipeline([\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid6 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': np.arange(0.1, 0.55, 0.05),\n",
    "              'classifier__skip_drop': np.arange(0.1, 0.55, 0.05),\n",
    "               'classifier__learning_rate':[grid2.best_params_['classifier__learning_rate']], # \n",
    "             'classifier__n_estimators':[grid5.best_params_['classifier__n_estimators']], # \n",
    "               'classifier__max_depth': [grid2.best_params_['classifier__max_depth']], # 3\n",
    "               'classifier__min_child_weight': [grid2.best_params_['classifier__min_child_weight']],# 3\n",
    "               'classifier__gamma':[grid3.best_params_['classifier__gamma']], # 0.5\n",
    "               'classifier__subsample':[grid4.best_params_['classifier__subsample']], # 0.8\n",
    "               'classifier__colsample_bytree':[grid4.best_params_['classifier__colsample_bytree']], # 0.8\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1],\n",
    "              }\n",
    "             ]\n",
    "grid6 = GridSearchCV(pipe, param_grid6, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid6.fit(X_train, y_train)\n",
    "print(grid6.best_params_)\n",
    "print(grid6.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-module",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "disabled-favorite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 29.8min\n",
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed: 37.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': XGBClassifier(alpha=0.0001, base_score=None, booster='dart',\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, eval_metric='error', gamma=0.0, gpu_id=None,\n",
      "              importance_type='gain', interaction_constraints=None, lambda=1,\n",
      "              learning_rate=0.02, max_delta_step=None, max_depth=9,\n",
      "              min_child_weight=4, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=1000, n_jobs=-1, nthread=-1, num_parallel_tree=None,\n",
      "              objective='binary:logistic', random_state=None,\n",
      "              rate_drop=0.20000000000000004, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=1, seed=2021, skip_drop=0.30000000000000004, ...), 'classifier__alpha': 0.0001, 'classifier__booster': 'dart', 'classifier__colsample_bytree': 0.8, 'classifier__eval_metric': 'error', 'classifier__gamma': 0.0, 'classifier__lambda': 1, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 9, 'classifier__min_child_weight': 4, 'classifier__n_estimators': 1000, 'classifier__n_jobs': -1, 'classifier__nthread': -1, 'classifier__objective': 'binary:logistic', 'classifier__rate_drop': 0.20000000000000004, 'classifier__scale_pos_weight': 1, 'classifier__seed': 2021, 'classifier__skip_drop': 0.30000000000000004, 'classifier__subsample': 0.8}\n",
      "0.8221052631578948\n"
     ]
    }
   ],
   "source": [
    "#   \n",
    "\n",
    "pipe = Pipeline([\n",
    "                ('classifier', XGBClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid7 = [              \n",
    "              {'classifier': [XGBClassifier()],\n",
    "              'classifier__booster': ['dart'],\n",
    "              'classifier__rate_drop': [grid6.best_params_['classifier__rate_drop']],\n",
    "              'classifier__skip_drop': [grid6.best_params_['classifier__skip_drop']],\n",
    "               'classifier__learning_rate':[grid2.best_params_['classifier__learning_rate']], # \n",
    "             'classifier__n_estimators':[grid5.best_params_['classifier__n_estimators']], # \n",
    "               'classifier__max_depth': [grid2.best_params_['classifier__max_depth']], # 3\n",
    "               'classifier__min_child_weight': [grid2.best_params_['classifier__min_child_weight']],# 3\n",
    "               'classifier__gamma':[grid3.best_params_['classifier__gamma']], # 0.5\n",
    "               'classifier__subsample':[grid4.best_params_['classifier__subsample']], # 0.8\n",
    "               'classifier__colsample_bytree':[grid4.best_params_['classifier__colsample_bytree']], # 0.8\n",
    "               'classifier__objective':['binary:logistic'],\n",
    "               'classifier__nthread':[-1],\n",
    "               'classifier__scale_pos_weight':[1],\n",
    "               'classifier__seed':[2021],\n",
    "               'classifier__eval_metric':['error'],\n",
    "               'classifier__n_jobs':[-1],\n",
    "               'classifier__alpha':[0, 1e-4, 1e-3, 1e-2, 0.1, 1],\n",
    "               'classifier__lambda':[1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100]\n",
    "              }\n",
    "             ]\n",
    "grid7 = GridSearchCV(pipe, param_grid7, scoring = 'accuracy',\n",
    "                    cv=StratifiedKFold(n_splits=5),\n",
    "                    verbose=1, n_jobs=-1)\n",
    "grid7.fit(X_train, y_train)\n",
    "print(grid7.best_params_)\n",
    "print(grid7.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-lingerie",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "innovative-radical",
   "metadata": {},
   "source": [
    "# Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cleared-brook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/(10, 0.8215488215488216)\n",
      "1/(20, 0.835016835016835)\n",
      "2/3/4/5/6/7/8/9/10/11/12/13/14/15/16/17/18/19/20/21/22/23/24/25/26/27/28/29/30/31/32/33/34/35/36/37/(380, 0.8417508417508418)\n",
      "38/39/40/41/42/43/44/45/46/47/48/49/"
     ]
    }
   ],
   "source": [
    "xgb_best = grid7.best_params_['classifier']\n",
    "best_esr_stoprounds_rfe = (-1, -1)\n",
    "for i, esr in enumerate(np.arange(10, 510, 10)):\n",
    "    print(i, end='/')\n",
    "    xgb_best = grid7.best_params_['classifier']\n",
    "    xgb_best.fit(X_train, y_train, early_stopping_rounds=esr, eval_metric=\"error\",\n",
    "                 eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=0)\n",
    "    acc = accuracy_score(y_test, xgb_best.predict(X_test))\n",
    "    \n",
    "    if acc > best_esr_stoprounds_rfe[1]:\n",
    "        best_esr_stoprounds_rfe = (esr, acc)\n",
    "        print(best_esr_stoprounds_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-aspect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-stephen",
   "metadata": {},
   "source": [
    "# Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "double-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lof_xgb2(model, df, stoprounds=50,\n",
    "                  scaler=None, poly=None, dim_reduction=None, rfe=None,\n",
    "                  preset=False):    \n",
    "    best_params, best_acc = 0, 0  \n",
    "    test_neighbors = np.linspace(1, 30, num=30).astype(int)\n",
    "    test_contams = np.linspace(0.01, 0.3, num=30)\n",
    "    metrics = ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
    "      'manhattan']\n",
    "    \n",
    "    if preset:\n",
    "        X0_train, X0_valid, X0_test, y0_train, y0_valid, y0_test = df\n",
    "        \n",
    "    else:\n",
    "        X0 = df.drop('sold', axis=1)\n",
    "        y0 = df.sold\n",
    "        X0_train, X0_test, y0_train, y0_test = train_test_split(X0, y0,\n",
    "                                                                test_size=0.2,\n",
    "                                                                shuffle=True,\n",
    "                                                                stratify=y0,\n",
    "                                                                random_state=11)\n",
    "        X0_train, X0_valid, y0_train, y0_valid = train_test_split(X0_train, y0_train,\n",
    "                                                                test_size=0.2,\n",
    "                                                                shuffle=True,\n",
    "                                                                stratify=y0_train,\n",
    "                                                                random_state=11)\n",
    "\n",
    "        if scaler:\n",
    "            X0_train = scaler.fit_transform(X0_train)\n",
    "            X0_valid = scaler.transform(X0_valid)\n",
    "            X0_test = scaler.transform(X0_test)\n",
    "\n",
    "        if poly:\n",
    "            X0_train = poly.fit_transform(X0_train)\n",
    "            X0_valid = poly.transform(X0_valid)\n",
    "            X0_test = poly.transform(X0_test)\n",
    "\n",
    "        if dim_reduction:\n",
    "            X0_train = dim_reduction.fit_transform(X0_train)\n",
    "            X0_valid = dim_reduction.transform(X0_valid)\n",
    "            X0_test = dim_reduction.transform(X0_test)\n",
    "\n",
    "        if rfe:\n",
    "            X0_train = rfe.fit_transform(X0_train, y0_train)\n",
    "            X0_valid = rfe.transform(X0_valid)\n",
    "            X0_test = rfe.transform(X0_test)\n",
    "\n",
    "        print('preprocessing complete')\n",
    "    \n",
    "    for i, tn in enumerate(test_neighbors):\n",
    "        for j, tc in enumerate(test_contams):\n",
    "            print(i, j, end=' / ')\n",
    "            #for m in metrics:\n",
    "            \n",
    "            #     \n",
    "            X_train_copy, X_valid_copy, X_test_copy = X0_train.copy(), X0_valid.copy(), X0_test.copy()\n",
    "            y_train_copy, y_valid_copy, y_test_copy = y0_train.copy(), y0_valid.copy(), y0_test.copy()\n",
    "\n",
    "            # LOF     \n",
    "            clf = LocalOutlierFactor(n_neighbors=tn, contamination=tc,\n",
    "                                    novelty=True, n_jobs=-1)\n",
    "            clf.fit(X_train_copy)\n",
    "\n",
    "            #   \n",
    "            y_pred = clf.predict(X_train_copy)\n",
    "            lof_outlier_idx_train = pd.Series(y_pred)[pd.Series(y_pred)==-1].index\n",
    "            X_train_copy = pd.DataFrame(X_train_copy).drop(lof_outlier_idx_train)\n",
    "            y_train_copy = y_train_copy.reset_index(drop=True).drop(lof_outlier_idx_train)\n",
    "\n",
    "            #    \n",
    "            yval_pred = clf.predict(X_valid_copy)\n",
    "            lof_outlier_idx_valid = pd.Series(yval_pred)[pd.Series(yval_pred)==-1].index\n",
    "            X_valid_copy = pd.DataFrame(X_valid_copy).drop(lof_outlier_idx_valid)\n",
    "            y_valid_copy = y_valid_copy.reset_index(drop=True).drop(lof_outlier_idx_valid)\n",
    "\n",
    "            #    \n",
    "            ytest_pred = clf.predict(X_test_copy)\n",
    "            lof_outlier_idx_test = pd.Series(ytest_pred)[pd.Series(ytest_pred)==-1].index\n",
    "            X_test_copy = pd.DataFrame(X_test_copy).drop(lof_outlier_idx_test)\n",
    "            y_test_copy = y_test_copy.reset_index(drop=True).drop(lof_outlier_idx_test)\n",
    "\n",
    "            #    /  \n",
    "            mod = model\n",
    "            mod.fit(X_train_copy, y_train_copy, early_stopping_rounds=stoprounds, eval_metric=\"error\",\n",
    "                 eval_set=[(X_train_copy, y_train_copy), (X_valid_copy, y_valid_copy)], verbose=0)\n",
    "\n",
    "            #      \n",
    "            mod_acc = accuracy_score(y_test_copy, mod.predict(X_test_copy))\n",
    "            if best_acc < mod_acc:\n",
    "                best_acc = mod_acc\n",
    "                best_params = (tn, tc)\n",
    "#                 X2 = X2\n",
    "#                 y2 = y2\n",
    "                print((tn, tc, best_acc))\n",
    "    \n",
    "    return {'best_params':best_params,\n",
    "           'best_accuracy':best_acc,\n",
    "           'preprocessed_data':[X0_train, X0_valid, X0_test, y0_train, y0_valid, y0_test],\n",
    "           'LOF_data':[X_train_copy, X_valid_copy, X_test_copy,\n",
    "                      y_train_copy, y_valid_copy, y_test_copy]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ultimate-california",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.0001, base_score=0.5, booster='dart', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, eval_metric='error',\n",
       "              gamma=0.0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', lambda=1, learning_rate=0.02,\n",
       "              max_delta_step=0, max_depth=9, min_child_weight=4, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=1000, n_jobs=-1,\n",
       "              nthread=-1, num_parallel_tree=1, objective='binary:logistic',\n",
       "              random_state=2021, rate_drop=0.20000000000000004,\n",
       "              reg_alpha=9.99999975e-05, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=2021, skip_drop=0.30000000000000004, ...)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid7.best_params_['classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "absent-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best = XGBClassifier(alpha=0.0001, base_score=0.5, booster='dart', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.8, eval_metric='error',\n",
    "              gamma=0.0, gpu_id=-1, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.02,\n",
    "              max_delta_step=0, max_depth=9, min_child_weight=4,\n",
    "              monotone_constraints='()', n_estimators=1000, n_jobs=-1,\n",
    "              nthread=-1, num_parallel_tree=1, objective='binary:logistic',\n",
    "              random_state=2021, rate_drop=0.20000000000000004,\n",
    "              reg_alpha=9.99999975e-05, reg_lambda=1, scale_pos_weight=1,\n",
    "              seed=2021, skip_drop=0.30000000000000004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "isolated-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 / (1, 0.01, 0.8175675675675675)\n",
      "0 1 / (1, 0.019999999999999997, 0.8191126279863481)\n",
      "0 2 / 0 3 / 0 4 / (1, 0.049999999999999996, 0.8204225352112676)\n",
      "0 5 / 0 6 / 0 7 / 0 8 / 0 9 / 0 10 / 0 11 / 0 12 / 0 13 / 0 14 / 0 15 / 0 16 / 0 17 / (1, 0.18, 0.8210116731517509)\n",
      "0 18 / (1, 0.18999999999999997, 0.82421875)\n",
      "0 19 / (1, 0.19999999999999998, 0.8260869565217391)\n",
      "0 20 / (1, 0.20999999999999996, 0.8285714285714286)\n",
      "0 21 / (1, 0.21999999999999997, 0.8319672131147541)\n",
      "0 22 / (1, 0.22999999999999998, 0.8326359832635983)\n",
      "0 23 / 0 24 / (1, 0.24999999999999997, 0.8333333333333334)\n",
      "0 25 / (1, 0.25999999999999995, 0.8362068965517241)\n",
      "0 26 / (1, 0.26999999999999996, 0.8384279475982532)\n",
      "0 27 / (1, 0.27999999999999997, 0.8392857142857143)\n",
      "0 28 / (1, 0.29, 0.8423423423423423)\n",
      "0 29 / 1 0 / 1 1 / 1 2 / 1 3 / 1 4 / 1 5 / 1 6 / 1 7 / 1 8 / 1 9 / 1 10 / (2, 0.10999999999999997, 0.8446969696969697)\n",
      "1 11 / (2, 0.11999999999999998, 0.8461538461538461)\n",
      "1 12 / 1 13 / 1 14 / 1 15 / (2, 0.15999999999999998, 0.856)\n",
      "1 16 / 1 17 / (2, 0.18, 0.8669354838709677)\n",
      "1 18 / 1 19 / 1 20 / 1 21 / 1 22 / 1 23 / 1 24 / 1 25 / 1 26 / 1 27 / 1 28 / 1 29 / 2 0 / 2 1 / 2 2 / 2 3 / 2 4 / 2 5 / 2 6 / 2 7 / 2 8 / 2 9 / 2 10 / 2 11 / 2 12 / 2 13 / 2 14 / 2 15 / 2 16 / 2 17 / 2 18 / 2 19 / 2 20 / 2 21 / 2 22 / 2 23 / 2 24 / 2 25 / 2 26 / 2 27 / 2 28 / 2 29 / 3 0 / 3 1 / 3 2 / 3 3 / 3 4 / 3 5 / 3 6 / 3 7 / 3 8 / 3 9 / 3 10 / 3 11 / 3 12 / 3 13 / 3 14 / 3 15 / 3 16 / 3 17 / 3 18 / 3 19 / 3 20 / 3 21 / 3 22 / 3 23 / 3 24 / 3 25 / 3 26 / 3 27 / 3 28 / 3 29 / 4 0 / 4 1 / 4 2 / 4 3 / 4 4 / 4 5 / 4 6 / 4 7 / 4 8 / 4 9 / 4 10 / 4 11 / 4 12 / 4 13 / 4 14 / 4 15 / 4 16 / 4 17 / 4 18 / 4 19 / 4 20 / 4 21 / 4 22 / 4 23 / 4 24 / 4 25 / 4 26 / 4 27 / 4 28 / 4 29 / 5 0 / 5 1 / 5 2 / 5 3 / 5 4 / 5 5 / 5 6 / 5 7 / 5 8 / 5 9 / 5 10 / 5 11 / 5 12 / 5 13 / 5 14 / 5 15 / 5 16 / 5 17 / 5 18 / 5 19 / 5 20 / 5 21 / 5 22 / 5 23 / 5 24 / 5 25 / 5 26 / 5 27 / 5 28 / 5 29 / 6 0 / 6 1 / 6 2 / 6 3 / 6 4 / 6 5 / 6 6 / 6 7 / 6 8 / 6 9 / 6 10 / 6 11 / 6 12 / 6 13 / 6 14 / 6 15 / 6 16 / 6 17 / 6 18 / 6 19 / 6 20 / 6 21 / 6 22 / 6 23 / 6 24 / 6 25 / 6 26 / 6 27 / 6 28 / 6 29 / 7 0 / 7 1 / 7 2 / 7 3 / 7 4 / 7 5 / 7 6 / 7 7 / 7 8 / 7 9 / 7 10 / 7 11 / 7 12 / 7 13 / 7 14 / 7 15 / 7 16 / 7 17 / 7 18 / 7 19 / 7 20 / 7 21 / 7 22 / 7 23 / 7 24 / 7 25 / 7 26 / 7 27 / 7 28 / 7 29 / 8 0 / 8 1 / 8 2 / 8 3 / 8 4 / 8 5 / 8 6 / 8 7 / 8 8 / 8 9 / 8 10 / 8 11 / 8 12 / 8 13 / 8 14 / 8 15 / 8 16 / 8 17 / 8 18 / 8 19 / 8 20 / 8 21 / 8 22 / 8 23 / 8 24 / 8 25 / 8 26 / 8 27 / 8 28 / 8 29 / 9 0 / 9 1 / 9 2 / 9 3 / 9 4 / 9 5 / 9 6 / 9 7 / 9 8 / 9 9 / 9 10 / 9 11 / 9 12 / 9 13 / 9 14 / 9 15 / 9 16 / 9 17 / 9 18 / 9 19 / 9 20 / 9 21 / 9 22 / 9 23 / 9 24 / 9 25 / 9 26 / 9 27 / 9 28 / 9 29 / 10 0 / 10 1 / 10 2 / 10 3 / 10 4 / 10 5 / 10 6 / 10 7 / 10 8 / 10 9 / 10 10 / 10 11 / 10 12 / 10 13 / 10 14 / 10 15 / 10 16 / 10 17 / 10 18 / 10 19 / 10 20 / 10 21 / 10 22 / 10 23 / 10 24 / 10 25 / 10 26 / 10 27 / 10 28 / 10 29 / 11 0 / 11 1 / 11 2 / 11 3 / 11 4 / 11 5 / 11 6 / 11 7 / 11 8 / 11 9 / 11 10 / 11 11 / 11 12 / 11 13 / 11 14 / 11 15 / 11 16 / 11 17 / 11 18 / 11 19 / 11 20 / 11 21 / 11 22 / 11 23 / 11 24 / 11 25 / 11 26 / 11 27 / 11 28 / 11 29 / 12 0 / 12 1 / 12 2 / 12 3 / 12 4 / 12 5 / 12 6 / 12 7 / 12 8 / 12 9 / 12 10 / 12 11 / 12 12 / 12 13 / 12 14 / 12 15 / 12 16 / 12 17 / 12 18 / 12 19 / 12 20 / 12 21 / 12 22 / 12 23 / 12 24 / 12 25 / 12 26 / 12 27 / 12 28 / 12 29 / 13 0 / 13 1 / 13 2 / 13 3 / 13 4 / 13 5 / 13 6 / 13 7 / 13 8 / 13 9 / 13 10 / 13 11 / 13 12 / 13 13 / 13 14 / 13 15 / 13 16 / 13 17 / 13 18 / 13 19 / 13 20 / 13 21 / 13 22 / 13 23 / 13 24 / 13 25 / 13 26 / 13 27 / 13 28 / 13 29 / 14 0 / 14 1 / 14 2 / 14 3 / 14 4 / 14 5 / 14 6 / 14 7 / 14 8 / 14 9 / 14 10 / 14 11 / 14 12 / 14 13 / 14 14 / 14 15 / 14 16 / 14 17 / 14 18 / 14 19 / 14 20 / 14 21 / 14 22 / 14 23 / 14 24 / 14 25 / 14 26 / 14 27 / 14 28 / 14 29 / 15 0 / 15 1 / 15 2 / 15 3 / 15 4 / 15 5 / 15 6 / 15 7 / 15 8 / 15 9 / 15 10 / 15 11 / 15 12 / 15 13 / 15 14 / 15 15 / 15 16 / 15 17 / 15 18 / 15 19 / 15 20 / 15 21 / 15 22 / 15 23 / 15 24 / 15 25 / 15 26 / 15 27 / 15 28 / 15 29 / 16 0 / 16 1 / 16 2 / 16 3 / 16 4 / 16 5 / 16 6 / 16 7 / 16 8 / 16 9 / 16 10 / 16 11 / 16 12 / 16 13 / 16 14 / 16 15 / 16 16 / 16 17 / 16 18 / 16 19 / 16 20 / 16 21 / 16 22 / 16 23 / 16 24 / 16 25 / 16 26 / 16 27 / 16 28 / 16 29 / 17 0 / 17 1 / 17 2 / 17 3 / 17 4 / 17 5 / 17 6 / 17 7 / 17 8 / 17 9 / 17 10 / 17 11 / 17 12 / 17 13 / 17 14 / 17 15 / 17 16 / 17 17 / 17 18 / 17 19 / 17 20 / 17 21 / 17 22 / 17 23 / 17 24 / 17 25 / 17 26 / 17 27 / 17 28 / 17 29 / 18 0 / 18 1 / 18 2 / 18 3 / 18 4 / 18 5 / 18 6 / 18 7 / 18 8 / 18 9 / 18 10 / 18 11 / 18 12 / 18 13 / 18 14 / 18 15 / 18 16 / 18 17 / 18 18 / 18 19 / 18 20 / 18 21 / 18 22 / 18 23 / 18 24 / 18 25 / 18 26 / 18 27 / 18 28 / 18 29 / 19 0 / 19 1 / 19 2 / 19 3 / 19 4 / 19 5 / 19 6 / 19 7 / 19 8 / 19 9 / 19 10 / 19 11 / 19 12 / 19 13 / 19 14 / 19 15 / 19 16 / 19 17 / 19 18 / 19 19 / 19 20 / 19 21 / 19 22 / 19 23 / 19 24 / 19 25 / 19 26 / 19 27 / 19 28 / 19 29 / 20 0 / 20 1 / 20 2 / 20 3 / 20 4 / 20 5 / 20 6 / 20 7 / 20 8 / 20 9 / 20 10 / 20 11 / 20 12 / 20 13 / 20 14 / 20 15 / 20 16 / 20 17 / 20 18 / 20 19 / 20 20 / 20 21 / 20 22 / 20 23 / 20 24 / 20 25 / 20 26 / 20 27 / 20 28 / 20 29 / 21 0 / 21 1 / 21 2 / 21 3 / 21 4 / 21 5 / 21 6 / 21 7 / 21 8 / 21 9 / 21 10 / 21 11 / 21 12 / 21 13 / 21 14 / 21 15 / 21 16 / 21 17 / 21 18 / 21 19 / 21 20 / 21 21 / 21 22 / 21 23 / 21 24 / 21 25 / 21 26 / 21 27 / 21 28 / 21 29 / 22 0 / 22 1 / 22 2 / 22 3 / 22 4 / 22 5 / 22 6 / 22 7 / 22 8 / 22 9 / 22 10 / 22 11 / 22 12 / 22 13 / 22 14 / 22 15 / 22 16 / 22 17 / 22 18 / 22 19 / 22 20 / 22 21 / 22 22 / 22 23 / 22 24 / 22 25 / 22 26 / 22 27 / 22 28 / 22 29 / 23 0 / 23 1 / 23 2 / 23 3 / 23 4 / 23 5 / 23 6 / 23 7 / 23 8 / 23 9 / 23 10 / 23 11 / 23 12 / 23 13 / 23 14 / 23 15 / 23 16 / 23 17 / 23 18 / 23 19 / 23 20 / 23 21 / 23 22 / 23 23 / 23 24 / 23 25 / 23 26 / 23 27 / 23 28 / 23 29 / 24 0 / 24 1 / 24 2 / 24 3 / 24 4 / 24 5 / 24 6 / 24 7 / 24 8 / 24 9 / 24 10 / 24 11 / 24 12 / 24 13 / 24 14 / 24 15 / 24 16 / 24 17 / 24 18 / 24 19 / 24 20 / 24 21 / 24 22 / 24 23 / 24 24 / 24 25 / 24 26 / 24 27 / 24 28 / 24 29 / 25 0 / 25 1 / 25 2 / 25 3 / 25 4 / 25 5 / 25 6 / 25 7 / 25 8 / 25 9 / 25 10 / 25 11 / 25 12 / 25 13 / 25 14 / 25 15 / 25 16 / 25 17 / 25 18 / 25 19 / 25 20 / 25 21 / 25 22 / 25 23 / 25 24 / 25 25 / 25 26 / 25 27 / 25 28 / 25 29 / 26 0 / 26 1 / 26 2 / 26 3 / 26 4 / 26 5 / 26 6 / 26 7 / 26 8 / 26 9 / 26 10 / 26 11 / 26 12 / 26 13 / 26 14 / 26 15 / 26 16 / 26 17 / 26 18 / 26 19 / 26 20 / 26 21 / 26 22 / 26 23 / 26 24 / 26 25 / 26 26 / 26 27 / 26 28 / 26 29 / 27 0 / 27 1 / 27 2 / 27 3 / 27 4 / 27 5 / 27 6 / 27 7 / 27 8 / 27 9 / 27 10 / 27 11 / 27 12 / 27 13 / 27 14 / 27 15 / 27 16 / 27 17 / 27 18 / 27 19 / 27 20 / 27 21 / 27 22 / 27 23 / 27 24 / 27 25 / 27 26 / 27 27 / 27 28 / 27 29 / 28 0 / 28 1 / 28 2 / 28 3 / 28 4 / 28 5 / 28 6 / 28 7 / 28 8 / 28 9 / 28 10 / 28 11 / 28 12 / 28 13 / 28 14 / 28 15 / 28 16 / 28 17 / 28 18 / 28 19 / 28 20 / 28 21 / 28 22 / 28 23 / 28 24 / 28 25 / 28 26 / 28 27 / 28 28 / 28 29 / 29 0 / 29 1 / 29 2 / 29 3 / 29 4 / 29 5 / 29 6 / 29 7 / 29 8 / 29 9 / 29 10 / 29 11 / 29 12 / 29 13 / 29 14 / 29 15 / 29 16 / 29 17 / 29 18 / 29 19 / 29 20 / 29 21 / 29 22 / 29 23 / 29 24 / 29 25 / 29 26 / 29 27 / 29 28 / 29 29 / "
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2, 0.18), 0.8669354838709677)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xgb_best = grid7.best_params_['classifier']\n",
    "\n",
    "xgb_scaler = MinMaxScaler()\n",
    "xgb_poly = PolynomialFeatures(degree=3)\n",
    "xgb_rfe = RFE(XGBClassifier(objective='binary:logistic',\n",
    "                           eval_metric='error'),\n",
    "                          n_features_to_select=70)\n",
    "\n",
    "df = (X_train.copy(), X_valid.copy(), X_test.copy(),\n",
    "      y_train.copy(), y_valid.copy(), y_test.copy())\n",
    "\n",
    "xgb_lof_tune = tune_lof_xgb2(xgb_best, df,\n",
    "                              #stoprounds=best_esr_stoprounds_rfe[0],\n",
    "                              stoprounds=380,\n",
    "                              scaler=xgb_scaler,\n",
    "                              poly=xgb_poly,\n",
    "                              rfe=xgb_rfe,\n",
    "                             preset=True)\n",
    "\n",
    "xgb_lof_tune['best_params'], xgb_lof_tune['best_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-sample",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-burning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
