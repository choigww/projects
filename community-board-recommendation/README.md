# 인터넷 커뮤니티 및 게시판 추천 시스템

관심사 키워드 + Fit 기반 웹사이트 추천



![k-communities-by-size](https://image.fmkorea.com/files/attach/new/20200406/486616/1013740883/2858255985/aca45d76f6004dc56b5a5b0c674532b2.png)

![coms-by-size](https://t1.daumcdn.net/cfile/tistory/2638D53958D3CAE634)



## Problem

- 코로나19로 가속화된 온라인 생활, 그 중심에 있는 인터넷 커뮤니티
- 각각의 인터넷 커뮤니티는 매우 다양한 관심사를 다루는 서브채널(게시판)이 존재
- 단순 관심사를 포함해서, 내 성격이나 생활 패턴 등 다양한 요소를 고려해주는 커뮤니티 추천이 필요해!



## Solution

- 문자열 기반 인터넷 커뮤니티 게시판 추천 시스템
- 크게 두 가지 프로세스로 진행
  - 감성분석 기반 커뮤니티 추천
    - 문자열 감성분석 점수 기반
    - 일 평균 게시물 수/ 부정어 빈도 비율 등 다양한 요소 반영
  - 토픽모델링 기반 게시판 추천
    - 사용자의 주요 관심사 카테고리 기반



## How To

- 텍스트 크롤링
  - 대표성을 갖는 커뮤니티 및 각 커뮤니티 게시판 선정
    - 크롤링 범위 설정
  - AWS 클라우드 이용한 텍스트 데이터 크롤링
- 텍스트 EDA
  - 커뮤니티에서 사용되는 언어 특질 파악
    - 커뮤니티 별 확인되는 특질 파악
- 텍스트 전처리
  - 커스텀 사전
    - 불용어
    - 인터넷 용어 표준화
    - 띄어쓰기, 문법적 오류 교정
      - 커뮤니티 각각에 대하여 필요?
      - 필요하다면, 커스텀 사전 제작을 가능한 효율적으로 할 방법?
      - **BENCHMARK - eKoNLPY 접근법 분석 및 모방**
        - 경제, 금융 분야의 용어들 지원 (4,202개 지원, 사용자 사전 추가도 가능)
        - 동의어 변환 기능 제공 (1,325 쌍의 동의어가 정의되어 있음)
        - 경제, 금융 분야에서 자주 사용되는 동사와 형용사를 기본형으로 변환하는 기능 제공 (1,291개 지원)
        - https://www.notion.so/b0d1db84d7004262bf199dec2ca91d64
  - POS Tagging -> 기본형 변환 -> 동의어 변환
    - 품사 태그는 어떤 것들을 포함?
  - 기본순서
    - normalization
      - 동일 형식으로 정규화
      - 용어의 동치 관계 정의
        - 복수-단수 / 고유명사-일반명사 등
    - tokenization
      - 문장 경계의 판별 : 문장 부호 (인용 부호?)
      - 문장 범위 기준? - 마침표, 느낌표, 물음표
      - 토큰화 규칙 미적용 대상의 판별 : 고유명사 등
      - 축약형 표현의 고려
    - stemming
      - 어형 변형 형태로부터 어간만 추출 / 분리
      - lemmatization과 추구하는 목적은 같으나 방법론이 다름
      - 적절하지 않은 경우에 대해서도 어간을 추출해 통일시키는 등 문제점 존재
      - 모든 경우에 대해 유용하지 않음을 유념
    - lemmatization
      - 여러 표현 형식으로 표현된 단일 의미형식/의미단위로 일원화
      - 의미론적X, 단순 구문, 문법적으로 그냥 원형 복원해주는 것
      - stemming - 정보검색에 가까움, lemmatization = 전산언어, 자연어처리에 가까움
      - 자연어처리 기법의 근간이 되는 형태소분석을 통해 진행
        - 한글 = 어간만 추출하면 형태가 무너지기 쉬움
        - lemmatization은 원형복원하므로 단어 형태 및 의미가 명확히 남음
      - am, are is -> be / boys boys' boy's -> boy
    - stopword removal
    - feature selection (문서 대표성 가진 최소한의 토큰 집단의 선정)
      - 자질 선택 기준
        - chi-square statistic
          - feature - target label 간 독립성 측정 (통계학)
          - 독립이면 그 자질은 범주화에 영향을 미치지 않는다고 판단
        - TF-IDF
          - Document Frequency Threshold, 기준값 넘는 것만 취사선택
        - Stopwords
        - ,,,
- 모델링
  - LDA
    - 모델이 학습한 적 없는 단어도 처리 가능하다는 장점
    - 모델을 클라우드 상에서 학습시킨 뒤, 웹으로 전송시키는 연습을 해 볼까?
      - 매일 일정 텍스트를 자동으로 크롤링한 다음, 모델을 업데이트?



### 기타

- 특수어
  - ㅋㅋ, ㅎㅎ, 이모티콘 등등?
- 유행에 매우 민감한 성향
  - 너무 많이 등장하는 단어는 제외
- 야민정음
- 무수한 오타 / 문법파괴 / 띄어쓰기 오류
- **감성분석에 대한 접근**
  - 기존 접근법 : 감성어 사전 기반?
    - 게시글의 감성 스코어
    - 댓글의 감성 스코어
  - 각 커뮤니티마다 고유하게 나타나는 비단어적 특성들
    - 이모티콘 : ㅇㅅㅇ, ㅡㅡ, ...
    - 종결어미 : '~냐', '~다', '~능'
    - 띄어쓰기 준수 경향성
    - 문장의 평균적인 길이



### 한국어

- 교착어 (aggluinative language)
- 어간 (명사/동사) + 기능어 (조사/어미) = 어절
- 상황에 맞게 문장을 활용하기 때문에, 형태소 품사에 대한 중의성 해소가 중요한 문제



### 한국 형태소 분석기

*어떤 것이 더 적합하다* 라고 판단하기 위한 기준은? 샘플 테스트?

1. [UTagger](http://nlplab.ulsan.ac.kr)
2. [KKma](http://kkma.snu.ac.kr) 
   1. 띄어쓰기 오류에 덜 민감
   2. 동적 프로그래밍을 통해 모든 가능한 후보군 생성
   3. 휴리스틱 및 히든 마르코프 모델 기반한 확률모델 사용
3. [MACH](http://cs.sungshin.ac.kr/~shim/demo/mach.html) 
   1. 형태소 사전이 지정한 인접 조건 검사만으로 형태소 분석
4. [KLT](http://nlp.kookmin.ac.kr) 
5. [**Hannanum**](http://semanticweb.kaist.ac.kr)
6. 1. 다양한 워크플로우에 적용 가능한 Plug-In 형태의 형태소 분석기
   2. 광범위한 한글 태그 종류를 제공
      1. 전처리
         1. 문장 경계 인식 / 필터링 / 자동 띄어쓰기
      2. 형태소 분석
         1. 어절 단위로 발생가능한 모든 형태소 분석 결과 생성
      3. 품사 태깅
         1. 가장 유망한 형태소 분석 결과 선택하여 품사 태깅
7. [**Arirang (Lucene)**](https://issues.apache.org/jira/browse/LUCENE-4956) 
   1. 루씬 검색엔진에 사용되는 언어 분석기 중 한글 검색을 위해 개발
      1. 루씬 - 한글/중국어/일본어를 하나의 분석기가 처리하여 허점이 많았음
      2. 한국 개발자들이 루씬으로 한글 형태소분석만을 위한 분석기를 개발
   2. 검색엔진에 특화된 한글 형태소 분석기 개발 목표
   3. 형태소 분석을 위한 말뭉치 학습과 사전 일부는 '21세기 세종계획' 성과 활용
8. [**Komoran**](http://shineware.tistory.com/entry/KOMORAN-ver-24)
   1. 여러 어절을 하나의 품사로 분석 가능함으로써 형태소 분석기의 적용 도메인에 따라 공백이 포함된 고유명사를 더 정확하게 분석할 수 있음 
   2. 자소 단위 TRIE 사전 구성으로 사전 재탐색 횟수를 최소화하여 사전 탐색 속도 향상 
   3. 한국어에서 나타나는 불규칙을 학습 코퍼스 내에서 자동으로 추출하여 불규칙 어절에 대한 분석 정확률을 높임
   4. TRIE 및 압축된 사전 정보를 사용하여 메모리 사용률을 최소화함 
   5. **사용자가 사전에 새로운 단어들을 추가 가능**
      1. **신조어 및 고유명사 분석 가능** 
9. [**Twitter-Korean-text**](https://github.com/twitter/twitter-korean-text)
   1. 스칼라로 쓰인 한국어 처리기 
   2. 현재 텍스트 정규화와 형태소 분석, 스테밍을 지원하고 있음 
   3. 짧은 트윗이나 비정형 소셜 미디어 텍스트에 특화되어 있음 





### 감성분석

- 분석단위
  - 단어 (유니그램)
  - 구 / n그램
  - 문장
- 해석 방법
  - Bag of Words 기반 단어들의 감성값 총합 계산
    - 비지도학습, 대신 외부 정보 의존
    - 정보검색론에서 사용하는 방법
  - 주석이 달린 어휘 (WordNet, SentiWordNet)
    - 비지도학습, 대신 사전 의존
    - 은톨로지 / 시소러스 등 어휘사전 기반 감성분석
  - 자연어 처리를 통한 구문 패턴 이용
    - POS Tagging 등 적용한 뒤 구문패턴 파악, 이용
  - 문장구조를 트리 구조로 만들어서 감성분석
- 감성 분류 - Feature Extraction
  - 부정 표현을 어떻게 다루는가?
    - I dislike this camera vs. I really like this camera
    - I like+ this new model vs. I do [not like+] this new model
      - not like를 하나의 단어로 취급
      - 유니그램 기반이라면 not_like 등 전처리 단계에서 묶어주기
    - 핵심은 부정어를 긍정어와 구분하는 것, 방법은 다양할 수 있음
  - 어떤 문법단위를 사용할 것인가?
    - 형용사만 사용 / 모든 단어 / 유니그램 vs. n그램



### 지도 vs. 비지도 vs. 준지도

- 지도학습
  - 라벨을 붙인 데이터로 학습
  - 기준치 알고리즘 (Baseline Algorithm)
    - 토큰화
      - 분리 어휘 항목으로 구두점 사용
      - 스테밍, 불용어 미적용
    - 특징 추출
      - 유니그램, 바이그램, POS, Feature Combination
    - 다른 분류기를 이용한 분류
      - Naive Bayes
      - MaxEnt
      - SVM
- 비지도학습
  - 라벨 없는 대신 사전 또는 어휘집 기반
  - 비지도 학습 기법
    - 말뭉치 기반 접근법
      - 한글은 공개된 범용 감성사전이 존재하지 않음 - 스스로 만들어야
      - 특정 카테고리의 텍스트 말뭉치에서 전문적 / 커스텀한 감성 사전 구축
      - 말뭉치를 그룹화하거나 쪼개서, 특정 단어는 부정어라는 학습데이터 또는 annotate된 코퍼스를 생성
      - 이러한 말뭉치 기반 감성어 사전을 만드는 것은 비지도 학습 및 지도학습에 매우 유용할 수 있음
- 준지도학습
  - 적은 양의 학습 데이터와 대량의 텍스트데이터가 존재할 때
  - 소량의 학습데이터로 분류기를 학습시킨 뒤,
  - 대량 텍스트데이터를 쪼개서 각각의 쪼개진 데이터를 학습된 모델로 예측하고
  - 예측된 결과를 실험자/평가자가 하나하나 확인하여 학습 데이터에 다시 포함시킴
  - 이러한 방법을 반복적으로 시행함
    - 2000년대부터 사용
    - 대량의 데이터가 소셜 미디어 상에 많이 존재하는 현재, 가능성 있는 방법 중 하나.



### Stanford CoreNLP : RNN-based 감성분석

- 회귀적 딥러닝 기반 접근
- 최고 수준의 감성 분류기로 인정
  - Pipeline 기반 아키텍쳐
    - 전처리 > 품사태깅 > 개체명 인식 > 문장 단위 트리구조 생성 > 감성분석
  - 전형적인 bag-of-words 접근보다, 파싱된 트리구조에 문장들을 저장
  - 감성을 분류할 때 문장 구조를 고려함
  - 문장 단위로 감성분석 결과를 측정, 문헌의 경우 문장 감성점수의 총합 등으로 계산
- 감성 트리뱅크
  - 스탠포드 CoreNLP 웹사이트 -> 온라인 데모 URL
    - 파랑 = 긍정, 빨강 = 부정
    - 루트가 최상단, 하부로 내려갈수록 
      - 테스트 청크 - 주격 OR 목적격?
      - 그 아래에는 명사구 OR 동사구?
      - 최하단에는 단어 레벨
  - 구문 분석 트리의 모든 수준에서, 해당 단어의 감정 표현에 대한 주석을 달아줌
    - 사전 기반의 감성 분석은 단어 레벨에서만 감성 분석이 이루어짐
    - Stanford CoreNLP는 단어, 구 등 다양한 레벨에서 감성을 판별한 뒤, 최종적으로 문장 단위 감성을 판별
  - 5단계 스키마 적용 (--, -, 0, +, ++)



### LingPipe : Logistic Regression 기반 감성분석

- 로지스틱 회귀분석을 이용한 감성분석기
- 2단계 계층적 분류기법 적용
  - 1단계 > 객관적 문장과 주관적 문장 분리
  - 2단계 > 객관적 문장 제거한 뒤 남는 문장에서 문서 극성 (감성) 분류 (노이즈 감소)
- 주요 특징
  - 영화 리뷰 데이터에서 추출한 유니그램 피쳐값 사용
  - 인접 문장들끼리는 비슷한 주관성-객관성(SO) 양극성을 갖는다고 가정
  - 주격 문장을 효율적으로 추출하기 위해 최소절단 알고리즘 사용
  - 최소 절 단위로 나눈 값(=문장)들을 갖고 반복적으로 주관성 분류기로 감성 분류



### SentiWordNet : 사전기반 감성분석

- https://wordnet.princeton.edu/
  - 오픈소스 약 11만 단어
- SentiWordNet synsets
  - 3개 레이블로 구성
    - Obj(s) - 객관적 True / False
    - Pos(s) - 긍정적 True / False
    - Neg(s) - 부정적 True / False
    - Synset 점수는 2^3 = 8개의 3진 분류법으로 결정됨 
    - 수작업으로 일일이 세 개의 분류값을 각 단어에 대해 점수를 내린 뒤 학습
  - 감성 점수 = 3개 라벨을 할당하는 분류기의 비율로 결정
    - 감성 점수는 긍정 점수에서 부정 점수를 뺀 값으로 최종 계산
    - -1(매우 부정) ~ 1 (매우 긍정)
  - (예시) Very comfortable, but straps go loose quickly
    - comfortable
      - positive : 0.75
      - objective : 0.25
      - negative : 0.0
    - loose
      - positive : 0.0
      - objective : 0.375
      - negative : 0.625
    - 총합 = positive
      - positive : 0.75
      - objective : 0.625
      - negative : 0.625
    - 단점
      - "very" 등 문맥적 정보 고려하지 않고 단어 레벨에서 감성분석
  - 이렇게 계산한 감성 스코어를 기계학습 기반 분류기의 피쳐로 활용할 수 있음



### Detailed Explanation for LDA

- Kmooc 텍스트마이닝 12주차
  - Variational Inference
    - LDA 잠재변수 추정기법
    - 원본 디리클레 분포를 variational distribution으로 표현
    - 두 분포 사이 거리를 의미하는 Kullback-Leibler Divergence를 최소화하는 잠재변수를 산출하는 알고리즘
    - 정확하지만 느림
  - 깁슨 샘플링
    - 무작위성 있지만 빠르다
    - 저차원 분포로부터 반복샘플링으로 고차원 분포와 유사한 수치를 갖는 분포들을 찾는 방식



### 다항 토픽 모델링

- LDA 문헌 모델링은 출현하는 단어들만 가지고 모델링을 진행
  - 다항 토픽 모델링은 **개체별 토픽의 추이를 분석하고자 제안되었음**
- 다항 토픽 모델
  - 가장 일반적인 것은 DMR = Dirichlet-multinomial Regression
  - Mimno, McCallum (2008)
  - 개체별 토픽 추이를 분석하고자 제안
- **문헌과 주제분포 기반으로 저자, 발행처, 참고문헌, 날짜 정보 등 문헌의 메타데이터 특성(feature)을 제3 파라미터로 설정하여 토픽을 도출하는 LDA 기반 토픽모델링 기법**



### Mallet for Python

https://stackoverflow.com/questions/60988425/python-gensim-mallet

- LDA
- Parallel LDA
  - 대용량 텍스트를 LDA 처리 시 속도 매우 느림
  - 멀티스레딩 기법 적용
- DMR LDA
  - 시간 변위로 토픽 변화 추이를 볼 때 사용
- Hierarchical LDA
  - 비지도학습 방식으로 토픽 모델링을 계층구조로 생성
  - 성능이 썩 좋지는 않음
- Labeled LDA
  - 지도학습 방법을 적용한 LDA
- Polylingual Topic Model
  - https://www.aclweb.org/anthology/P14-1110.pdf
- Hierarchical Pachinko Allocation Model (PAM)
  - http://akashgit.github.io/research/pam-naacl.pdf



### 영어 감성사전을 번역해서 쓰면 어떨까

- General Inquirer
  - https://www.wjh.harvard.edu/~inquirer
  - 182개 카테고리, 긍정어 1915단어, 부정어 2291단어
  - 연구목적에 무료로 이용 가능
- MPQA 오피니언 코퍼스
  - https://www.cs.pitt.edu/mpqa/subj_lexicon.html
  - 세계 언론의 10,000개 이상의 문장들
  - 주관성 어휘
  - 논쟁 어휘
    - 논쟁 패턴
    - 논쟁 타입을 대표하는 17개 파일들
  - GNU GPL (오픈 데이터)
- SentiWordNet
  - https://sentiwordnet.isti.cnr.it/
  - WordNet의 synsets와 계층구조 기반
  - WordNet 세 가지 감성지수에 각각의 synset을 할당함
    - 긍정
    - 부정
    - 객관성
- 기타 웹 검색 결과
  - 수천 개의 유용한 용어들을 가짐
  - 단점 > 도메인이나, 내용 의존 견해단어들을 찾기 어려움, 낮은 품질, 문맥과 맥락 없음
- 거대 말뭉치에서는 구문론이나 co-occurrence 패턴에 의존
  - 높은 보급률 위해 거대한 말뭉치 필요
  - 범위 의존적 성향 (긍정, 부정, 중립
  - Turney(2002), Yu & Hazivassiloglou(2003)
    - 단어 유사성을 이용한 문헌 극성 결정 기법 제안
    - 단어/구에 대한 의견방향의 극성 (긍정/부정) 할당
    - "감성 지속성"
      - 견해 단어들을 식별하기 위해, 연결사들에 관습 사용
      - "접속사는 같은 방향을 갖는 형용사에 결합"
        - (예) this car is beautiful and spacious.
      - 두 개의 결합된 형용사의 방향이 같은지 다른지를 결정
        - 군집화 > 두 세트의 단어 생산 (긍정, 부정)



### 커스텀 지표

사용자의 주관적 성향을 수치화하여 비교할 수 있는 지표를 정의

- 정치 주제가 자주 언급되는가
  - 진보 친화적인가
  - 보수 친화적인가
- 욕설 및 부정적 단어가 거리낌 없이 사용되는가
- **참고**
  - 관점 지향 감성 분석 (Aspect-oriented sentiment analysis)



### 생각해볼 것

- 사용자가 과거에 커뮤니티에 게시했던 글을 입력받으면 커뮤니티 또는 게시판을 추천해준다면?
  - 트레인/ 테스트 셋을 만들어 검증해볼 수 있음.



![politics-preference-by-community](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F021CFD405194C3EB43)



![politics-preferences-by-community2](http://cdn.ppomppu.co.kr/zboard/data3/2019/0305/m_20190305171426_aoxownnh.jpg)



